{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.2.4'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "from keras import preprocessing\n",
    "from keras import optimizers\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import SeparableConv1D, MaxPooling1D\n",
    "from keras.layers import LSTM, GRU\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Flatten\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"is it me, or does 'frank moss' look like a child molester?\", 'why wait that they finish college or university?  you do know that those are free of charge in most european countries, right?', \"or if you christians got off of little boys' asses...\", \"'undoubtedly'?  some would disagree and say jesus was a good jew, albeit a reformer, and that st. paul actually started christianity.   newlinechar  newlinechar  newlinechar  newlinechar just thought i'd throw that out there.\"]\n",
      "[\"to me, it looks more like he 'takes a wide stance' on public restroom sex.\", 'are they for foreign nationals? i know uk universities charge fees of around £10,000 a year for foreign students (the uk students pay around £3,000 a year plus taxation). even when the uk system was free for uk nationals i still think we charged non-eu members to join.', \"i'm sure there's never been an atheist pederast...ever.\", \"there are others who believe that the christian church didn't start until much later and invented several centuries worth of history.\"]\n",
      "Found 29620 sentences from TrainFrom Text\n",
      "Found 29620 sentences from TrainTo Text\n",
      "Found 521666 words from TrainFrom Text\n",
      "Found 479824 words from TrainTo Text\n"
     ]
    }
   ],
   "source": [
    "trainFromTextFile = \"train.FROM\"\n",
    "trainToTextFile   = \"train.TO\"\n",
    "trainFromText     = open(trainFromTextFile, 'r', encoding='utf-8').read().lower()\n",
    "trainToText       = open(trainToTextFile, 'r', encoding='utf-8').read().lower()\n",
    "trainFromSentenceTokens = re.split('\\n', trainFromText)\n",
    "trainToSentenceTokens   = re.split('\\n', trainToText)\n",
    "trainFromWordTokens = re.split(' |\\n', trainFromText)\n",
    "trainToWordTokens   = re.split(' |\\n', trainToText)\n",
    "print(trainFromSentenceTokens[1])\n",
    "print(trainToSentenceTokens[1])\n",
    "print('Found %s sentences from TrainFrom Text' %len(trainFromSentenceTokens))\n",
    "print('Found %s sentences from TrainTo Text' %len(trainToSentenceTokens))\n",
    "print('Found %s words from TrainFrom Text' %len(trainFromWordTokens))\n",
    "print('Found %s words from TrainTo Text' %len(trainToWordTokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 521666 words.\n",
      "Found 521566 sequences.\n",
      "521566\n",
      "Found 59671 unique tokens.\n",
      "Found 59671 unique words.\n"
     ]
    }
   ],
   "source": [
    "train_len = 100\n",
    "text_sequences = []\n",
    "for i in range(train_len,len(trainFromWordTokens)):\n",
    "    seq = trainFromWordTokens[i-train_len:i]\n",
    "    text_sequences.append(seq)\n",
    "\n",
    "print('Found %s words.' %len(trainFromWordTokens))\n",
    "print('Found %s sequences.' %len(text_sequences))\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(text_sequences)\n",
    "sequences = tokenizer.texts_to_sequences(text_sequences)\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=train_len)\n",
    "\n",
    "print(len(sequences))\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "word_count = tokenizer.word_counts\n",
    "nWords     = len(tokenizer.word_counts) + 1\n",
    "\n",
    "n_sequences = np.empty([len(sequences),train_len], dtype='int32')\n",
    "for i in range(len(sequences)):\n",
    "    n_sequences[i] = sequences[i]\n",
    "\n",
    "    \n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "print('Found %s unique words.' % len(word_count))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 34823 unique tokens.\n",
      "Found 34823 unique words.\n"
     ]
    }
   ],
   "source": [
    "tokenizer2 = Tokenizer()\n",
    "tokenizer2.fit_on_texts(trainFromSentenceTokens)\n",
    "sequences2 = tokenizer.texts_to_sequences(trainFromSentenceTokens)\n",
    "\n",
    "sequences2 = pad_sequences(sequences2, maxlen=train_len)\n",
    "\n",
    "word_index2 = tokenizer2.word_index\n",
    "word_count2 = tokenizer2.word_counts\n",
    "nWords     = len(tokenizer2.word_counts) + 1\n",
    "\n",
    "n_sequences2 = np.empty([len(sequences2),train_len], dtype='int32')\n",
    "for i in range(len(sequences2)):\n",
    "    n_sequences2[i] = sequences2[i]\n",
    "\n",
    "    \n",
    "print('Found %s unique tokens.' % len(word_index2))\n",
    "print('Found %s unique words.' % len(word_count2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29620\n",
      "29620\n"
     ]
    }
   ],
   "source": [
    "print(len(sequences2))\n",
    "print(len(n_sequences2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,   193,    30,     2, 20176,     8, 20177,\n",
       "         337,    17, 20178,     7,    95,   337,    71,    52,     4,\n",
       "          18,   496,   676,  4997,     8,  5716,    17,   133,  3414,\n",
       "         119])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences2[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,   193,    30,     2, 20176,     8, 20177,\n",
       "         337,    17, 20178,     7,    95,   337,    71,    52,     4,\n",
       "          18,   496,   676,  4997,     8,  5716,    17,   133,  3414,\n",
       "         119])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_sequences2[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 521666 words.\n",
      "Found 29620 sequences.\n",
      "Found 29620.\n",
      "Found 34823 unique tokens.\n",
      "Found 34823 unique words.\n"
     ]
    }
   ],
   "source": [
    "train_len = 100\n",
    "\n",
    "print('Found %s words.' %len(trainFromWordTokens))\n",
    "print('Found %s sequences.' %len(trainFromSentenceTokens))\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(trainFromSentenceTokens)\n",
    "sequences = tokenizer.texts_to_sequences(trainFromSentenceTokens)\n",
    "\n",
    "sequences = pad_sequences(sequences)\n",
    "\n",
    "print('Found %s.' %len(sequences))\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "word_count = tokenizer.word_counts\n",
    "nWords     = len(tokenizer.word_counts) + 1\n",
    "\n",
    "    \n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "print('Found %s unique words.' % len(word_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,   228,    29,     1, 15833,     7, 15834,   363,\n",
       "          15, 15835,     6, 15836,   111,   363,    65,    58,     3,\n",
       "          18,   415,   651,  4980,     7,  3587,    15,   151,  3156,\n",
       "          83])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'seems like the wildest and craziest guys are pastors of megachurches.  those guys know how to have fun.  republican senators and representatives are pretty wild too.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainFromSentenceTokens[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
