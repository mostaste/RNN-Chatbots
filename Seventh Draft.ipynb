{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.4'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "from keras import preprocessing\n",
    "from keras import optimizers\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers import SeparableConv1D, MaxPooling1D\n",
    "from keras.layers import Input\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import GRU\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Flatten\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 29620 sentences from TrainFrom Text\n",
      "Found 29620 sentences from TrainTo Text\n",
      "Found 521666 words from TrainFrom Text\n",
      "Found 479824 words from TrainTo Text\n"
     ]
    }
   ],
   "source": [
    "trainFromTextFile = \"train.FROM\"\n",
    "trainToTextFile   = \"train.TO\"\n",
    "trainFromText     = open(trainFromTextFile, 'r', encoding='utf-8').read().lower()\n",
    "trainToText       = open(trainToTextFile, 'r', encoding='utf-8').read().lower()\n",
    "trainFromSentence = re.split('\\n', trainFromText)\n",
    "trainToSentence   = re.split('\\n', trainToText)\n",
    "trainFromWords = re.split(' |\\n', trainFromText)\n",
    "trainToWords   = re.split(' |\\n', trainToText)\n",
    "\n",
    "print('Found %s sentences from TrainFrom Text' %len(trainFromSentence))\n",
    "print('Found %s sentences from TrainTo Text' %len(trainToSentence))\n",
    "print('Found %s words from TrainFrom Text' %len(trainFromWords))\n",
    "print('Found %s words from TrainTo Text' %len(trainToWords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "salad?\n",
      "jesus,\n",
      "does this also apply to the salad?\n",
      "jesus, i hope not.\n"
     ]
    }
   ],
   "source": [
    "print(trainFromWords[6])\n",
    "print(trainToWords[0])\n",
    "print(trainFromSentence[0])\n",
    "print(trainToSentence[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainInput = trainFromSentence[0:1000]\n",
    "trainTarget = trainToSentence[0:1000]\n",
    "\n",
    "len(trainTarget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train From File:\n",
      "\n",
      "Found 1000 sentences.\n",
      "Found 1000 sequences.\n",
      "Found 4304 unique tokens.\n",
      "Found 4304 unique words.\n"
     ]
    }
   ],
   "source": [
    "max_len = 100    # We will cut comments after 100 words\n",
    "#max_words = 10000  # We will only consider the top 10,000 words in the dataset\n",
    "\n",
    "tokenizerInput = Tokenizer()\n",
    "tokenizerInput.fit_on_texts(trainInput)\n",
    "\n",
    "sequencesInputEncode = tokenizerInput.texts_to_sequences(trainInput)\n",
    "sequencesInputEncode = pad_sequences(sequencesInputEncode, maxlen=max_len)  #Pad so all the arrays are the same size\n",
    "\n",
    "Inputindex = tokenizerInput.word_index\n",
    "Inputcount = tokenizerInput.word_counts\n",
    "nEncoderToken = len(tokenizerInput.word_index)+1\n",
    "\n",
    "trainInputEncoded = to_categorical([sequencesInputEncode])\n",
    "\n",
    "trainInputEncoded = trainInputEncoded.reshape(len(trainInput), max_len, nEncoderToken)\n",
    "\n",
    "print(\"Train From File:\\n\")\n",
    "print('Found %s sentences.' %len(trainInput))\n",
    "print('Found %s sequences.' %len(sequencesInputEncode))\n",
    "print('Found %s unique tokens.' % len(Inputindex))\n",
    "print('Found %s unique words.' % len(Inputcount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 100, 4305)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainInputEncoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizerTarget = Tokenizer()\n",
    "tokenizerTarget.fit_on_texts(trainTarget)\n",
    "\n",
    "sequencesInputDecode = tokenizerTarget.texts_to_sequences(trainTarget)\n",
    "sequencesTargetDecode = tokenizerTarget.texts_to_sequences(trainTarget)\n",
    "\n",
    "#Pops the First Element in the Sequence (To prepare for Decoder Target)\n",
    "for seq in sequencesTargetDecode:\n",
    "    if seq:\n",
    "        _ = seq.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[576, 5, 272, 15]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequencesInputDecode[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 272, 15]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequencesTargetDecode[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequencesInputDecode = pad_sequences(sequencesInputDecode, maxlen=max_len)  #Pad so all the arrays are the same size\n",
    "sequencesTargetDecode = pad_sequences(sequencesTargetDecode, maxlen=max_len)  #Pad so all the arrays are the same size\n",
    "Targetindex = tokenizerTarget.word_index\n",
    "Targetcount = tokenizerTarget.word_counts\n",
    "nDecoderToken = len(tokenizerTarget.word_index)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 1,\n",
       " 'newlinechar': 2,\n",
       " 'to': 3,\n",
       " 'a': 4,\n",
       " 'i': 5,\n",
       " 'you': 6,\n",
       " 'and': 7,\n",
       " 'of': 8,\n",
       " 'is': 9,\n",
       " 'that': 10,\n",
       " 'it': 11,\n",
       " 'in': 12,\n",
       " 'for': 13,\n",
       " 'on': 14,\n",
       " 'not': 15,\n",
       " 'they': 16,\n",
       " 'be': 17,\n",
       " 'have': 18,\n",
       " 'was': 19,\n",
       " 'are': 20,\n",
       " 'but': 21,\n",
       " 'if': 22,\n",
       " 'with': 23,\n",
       " \"it's\": 24,\n",
       " 'just': 25,\n",
       " 'my': 26,\n",
       " 'your': 27,\n",
       " 'or': 28,\n",
       " 'like': 29,\n",
       " 'as': 30,\n",
       " 'would': 31,\n",
       " 'all': 32,\n",
       " 'what': 33,\n",
       " 'this': 34,\n",
       " \"don't\": 35,\n",
       " 'so': 36,\n",
       " 'he': 37,\n",
       " 'http': 38,\n",
       " 'think': 39,\n",
       " 'people': 40,\n",
       " 'do': 41,\n",
       " 'me': 42,\n",
       " 'no': 43,\n",
       " 'at': 44,\n",
       " \"'\": 45,\n",
       " 'from': 46,\n",
       " 'one': 47,\n",
       " 'can': 48,\n",
       " 'gt': 49,\n",
       " 'how': 50,\n",
       " 'there': 51,\n",
       " 'about': 52,\n",
       " 'more': 53,\n",
       " 'we': 54,\n",
       " 'get': 55,\n",
       " 'know': 56,\n",
       " 'com': 57,\n",
       " 'some': 58,\n",
       " 'them': 59,\n",
       " 'then': 60,\n",
       " 'an': 61,\n",
       " \"that's\": 62,\n",
       " 'will': 63,\n",
       " 'up': 64,\n",
       " \"i'm\": 65,\n",
       " 'good': 66,\n",
       " 'well': 67,\n",
       " 'by': 68,\n",
       " 'who': 69,\n",
       " 'because': 70,\n",
       " 'make': 71,\n",
       " 'than': 72,\n",
       " 'their': 73,\n",
       " 'his': 74,\n",
       " 'too': 75,\n",
       " 'when': 76,\n",
       " 'www': 77,\n",
       " 'out': 78,\n",
       " 'why': 79,\n",
       " 'actually': 80,\n",
       " 'which': 81,\n",
       " 'reddit': 82,\n",
       " 'has': 83,\n",
       " 'should': 84,\n",
       " 'point': 85,\n",
       " 'only': 86,\n",
       " 'back': 87,\n",
       " 'time': 88,\n",
       " 'did': 89,\n",
       " 'see': 90,\n",
       " 'were': 91,\n",
       " 'go': 92,\n",
       " 'even': 93,\n",
       " 'still': 94,\n",
       " 'most': 95,\n",
       " 'now': 96,\n",
       " 'man': 97,\n",
       " \"you're\": 98,\n",
       " 'much': 99,\n",
       " 'could': 100,\n",
       " 'those': 101,\n",
       " \"didn't\": 102,\n",
       " 'other': 103,\n",
       " 'really': 104,\n",
       " 'mean': 105,\n",
       " 'used': 106,\n",
       " \"can't\": 107,\n",
       " \"doesn't\": 108,\n",
       " 'maybe': 109,\n",
       " 'any': 110,\n",
       " 'us': 111,\n",
       " 'first': 112,\n",
       " 'yes': 113,\n",
       " 'thing': 114,\n",
       " 'want': 115,\n",
       " 'been': 116,\n",
       " 'here': 117,\n",
       " 'does': 118,\n",
       " \"he's\": 119,\n",
       " 'yeah': 120,\n",
       " 'amp': 121,\n",
       " 'being': 122,\n",
       " 'right': 123,\n",
       " 'off': 124,\n",
       " 'comment': 125,\n",
       " 'someone': 126,\n",
       " 'got': 127,\n",
       " 'sure': 128,\n",
       " 'again': 129,\n",
       " 'read': 130,\n",
       " '2': 131,\n",
       " 'better': 132,\n",
       " 'down': 133,\n",
       " 'over': 134,\n",
       " 'many': 135,\n",
       " 'into': 136,\n",
       " 'true': 137,\n",
       " 'very': 138,\n",
       " 'day': 139,\n",
       " 'saying': 140,\n",
       " 'said': 141,\n",
       " 'watch': 142,\n",
       " 'thought': 143,\n",
       " 'guy': 144,\n",
       " 'had': 145,\n",
       " 'way': 146,\n",
       " 'everyone': 147,\n",
       " 'probably': 148,\n",
       " 'pretty': 149,\n",
       " 'our': 150,\n",
       " 'god': 151,\n",
       " 'sorry': 152,\n",
       " 'put': 153,\n",
       " 'where': 154,\n",
       " '1': 155,\n",
       " 'having': 156,\n",
       " 'youtube': 157,\n",
       " 'say': 158,\n",
       " 'thanks': 159,\n",
       " \"isn't\": 160,\n",
       " 'paul': 161,\n",
       " 'believe': 162,\n",
       " 'money': 163,\n",
       " 'call': 164,\n",
       " 'already': 165,\n",
       " 'oh': 166,\n",
       " 'least': 167,\n",
       " 'agree': 168,\n",
       " 'use': 169,\n",
       " 'without': 170,\n",
       " 'long': 171,\n",
       " 'same': 172,\n",
       " 'org': 173,\n",
       " 'two': 174,\n",
       " 'its': 175,\n",
       " 'free': 176,\n",
       " 'ever': 177,\n",
       " 'also': 178,\n",
       " 'tell': 179,\n",
       " 'life': 180,\n",
       " \"i'd\": 181,\n",
       " 'anyone': 182,\n",
       " 'take': 183,\n",
       " 'great': 184,\n",
       " 'might': 185,\n",
       " 'find': 186,\n",
       " 'always': 187,\n",
       " 'bad': 188,\n",
       " 'enough': 189,\n",
       " 'nothing': 190,\n",
       " 'though': 191,\n",
       " 'something': 192,\n",
       " 'these': 193,\n",
       " 'world': 194,\n",
       " 'give': 195,\n",
       " 'around': 196,\n",
       " 'system': 197,\n",
       " 'never': 198,\n",
       " \"they're\": 199,\n",
       " 'every': 200,\n",
       " 'look': 201,\n",
       " \"wasn't\": 202,\n",
       " 'little': 203,\n",
       " 'against': 204,\n",
       " 'things': 205,\n",
       " 'after': 206,\n",
       " 'guess': 207,\n",
       " 'o': 208,\n",
       " \"i've\": 209,\n",
       " 'government': 210,\n",
       " 'wait': 211,\n",
       " 'v': 212,\n",
       " 'while': 213,\n",
       " 'may': 214,\n",
       " 'code': 215,\n",
       " 'quite': 216,\n",
       " 'case': 217,\n",
       " 'exactly': 218,\n",
       " 'own': 219,\n",
       " 'made': 220,\n",
       " 'yet': 221,\n",
       " 'using': 222,\n",
       " 'reason': 223,\n",
       " 'him': 224,\n",
       " 'real': 225,\n",
       " 'job': 226,\n",
       " 'need': 227,\n",
       " 'seems': 228,\n",
       " 'such': 229,\n",
       " 'html': 230,\n",
       " 'wikipedia': 231,\n",
       " 'come': 232,\n",
       " 'work': 233,\n",
       " 'going': 234,\n",
       " 'norway': 235,\n",
       " 'wrong': 236,\n",
       " 'x': 237,\n",
       " \"there's\": 238,\n",
       " 'please': 239,\n",
       " 'makes': 240,\n",
       " 'doing': 241,\n",
       " 'both': 242,\n",
       " 'before': 243,\n",
       " 'buy': 244,\n",
       " 'write': 245,\n",
       " \"what's\": 246,\n",
       " 'kind': 247,\n",
       " 'part': 248,\n",
       " 'yourself': 249,\n",
       " 'stuff': 250,\n",
       " 'seen': 251,\n",
       " 'new': 252,\n",
       " 'bush': 253,\n",
       " 'starting': 254,\n",
       " 'years': 255,\n",
       " 'different': 256,\n",
       " 'once': 257,\n",
       " 'source': 258,\n",
       " 'fucking': 259,\n",
       " 'comments': 260,\n",
       " 'lot': 261,\n",
       " 'talking': 262,\n",
       " 'else': 263,\n",
       " 'under': 264,\n",
       " 'mysql': 265,\n",
       " 'likely': 266,\n",
       " 'oil': 267,\n",
       " 'article': 268,\n",
       " 'votes': 269,\n",
       " 'far': 270,\n",
       " 'books': 271,\n",
       " 'hope': 272,\n",
       " 'pay': 273,\n",
       " 'until': 274,\n",
       " 'order': 275,\n",
       " 'away': 276,\n",
       " 'she': 277,\n",
       " 'less': 278,\n",
       " 'through': 279,\n",
       " 'head': 280,\n",
       " 'trying': 281,\n",
       " 'freedom': 282,\n",
       " \"i'll\": 283,\n",
       " 'am': 284,\n",
       " 'video': 285,\n",
       " 'hard': 286,\n",
       " 's': 287,\n",
       " 'means': 288,\n",
       " 'story': 289,\n",
       " 'last': 290,\n",
       " 'ah': 291,\n",
       " \"we're\": 292,\n",
       " 'meant': 293,\n",
       " 'getting': 294,\n",
       " 'question': 295,\n",
       " 'argument': 296,\n",
       " 'old': 297,\n",
       " 'choice': 298,\n",
       " \"wouldn't\": 299,\n",
       " 'her': 300,\n",
       " 'fact': 301,\n",
       " 'fuck': 302,\n",
       " 'en': 303,\n",
       " 'wiki': 304,\n",
       " 'perhaps': 305,\n",
       " 'help': 306,\n",
       " 'clear': 307,\n",
       " 'keep': 308,\n",
       " 'joke': 309,\n",
       " 'understand': 310,\n",
       " 'kill': 311,\n",
       " 'car': 312,\n",
       " 'linux': 313,\n",
       " 'funny': 314,\n",
       " 'called': 315,\n",
       " 'instead': 316,\n",
       " 'support': 317,\n",
       " 'imagine': 318,\n",
       " \"you've\": 319,\n",
       " 'hell': 320,\n",
       " 'indeed': 321,\n",
       " 'book': 322,\n",
       " 'show': 323,\n",
       " 'wish': 324,\n",
       " 'left': 325,\n",
       " 'everything': 326,\n",
       " 'remember': 327,\n",
       " 'stop': 328,\n",
       " 'awesome': 329,\n",
       " 'lol': 330,\n",
       " 'lt': 331,\n",
       " 'literally': 332,\n",
       " 'days': 333,\n",
       " 'serious': 334,\n",
       " 'rather': 335,\n",
       " 'goes': 336,\n",
       " 'house': 337,\n",
       " 'making': 338,\n",
       " 'bit': 339,\n",
       " 'google': 340,\n",
       " '3d0': 341,\n",
       " 'change': 342,\n",
       " 'feel': 343,\n",
       " 'content': 344,\n",
       " 'p': 345,\n",
       " 'sense': 346,\n",
       " 'next': 347,\n",
       " 'gets': 348,\n",
       " 'roll': 349,\n",
       " \"'the\": 350,\n",
       " 'damn': 351,\n",
       " 'link': 352,\n",
       " 'thank': 353,\n",
       " 'net': 354,\n",
       " 'language': 355,\n",
       " 'thinking': 356,\n",
       " 'fun': 357,\n",
       " 'points': 358,\n",
       " 'thats': 359,\n",
       " 'consider': 360,\n",
       " 'c': 361,\n",
       " 'info': 362,\n",
       " 'comes': 363,\n",
       " 'ron': 364,\n",
       " 'place': 365,\n",
       " 'whole': 366,\n",
       " 'able': 367,\n",
       " 'big': 368,\n",
       " 'small': 369,\n",
       " 'pizza': 370,\n",
       " 'shit': 371,\n",
       " 'cool': 372,\n",
       " 'must': 373,\n",
       " 'game': 374,\n",
       " 'vi': 375,\n",
       " 'looks': 376,\n",
       " 'uk': 377,\n",
       " 'non': 378,\n",
       " 'start': 379,\n",
       " 'history': 380,\n",
       " 'knows': 381,\n",
       " 'second': 382,\n",
       " 'click': 383,\n",
       " 'says': 384,\n",
       " 'mention': 385,\n",
       " 'obviously': 386,\n",
       " 'top': 387,\n",
       " 'each': 388,\n",
       " 'anyway': 389,\n",
       " 'sir': 390,\n",
       " 'yep': 391,\n",
       " 'experience': 392,\n",
       " 'end': 393,\n",
       " 'mccain': 394,\n",
       " 'national': 395,\n",
       " 'news': 396,\n",
       " \"haven't\": 397,\n",
       " 'page': 398,\n",
       " 'version': 399,\n",
       " 'rule': 400,\n",
       " 'running': 401,\n",
       " 'friends': 402,\n",
       " 'hit': 403,\n",
       " 'myself': 404,\n",
       " 'america': 405,\n",
       " 'whether': 406,\n",
       " \"he'll\": 407,\n",
       " '3': 408,\n",
       " 'machine': 409,\n",
       " 'seem': 410,\n",
       " 'wanted': 411,\n",
       " 'country': 412,\n",
       " 'sheeple': 413,\n",
       " 'gonna': 414,\n",
       " 'except': 415,\n",
       " 'voted': 416,\n",
       " 'open': 417,\n",
       " 'heard': 418,\n",
       " 'basically': 419,\n",
       " 'parents': 420,\n",
       " 'run': 421,\n",
       " 'best': 422,\n",
       " \"won't\": 423,\n",
       " \"aren't\": 424,\n",
       " 'sounds': 425,\n",
       " 'word': 426,\n",
       " 'election': 427,\n",
       " 'gay': 428,\n",
       " 'behind': 429,\n",
       " 'jpg': 430,\n",
       " \"here's\": 431,\n",
       " 'post': 432,\n",
       " 'took': 433,\n",
       " '4': 434,\n",
       " 'etc': 435,\n",
       " '20': 436,\n",
       " 'definitely': 437,\n",
       " 'either': 438,\n",
       " 'face': 439,\n",
       " 'person': 440,\n",
       " \"couldn't\": 441,\n",
       " 'edit': 442,\n",
       " \"you'd\": 443,\n",
       " 'air': 444,\n",
       " \"let's\": 445,\n",
       " 'vote': 446,\n",
       " 'count': 447,\n",
       " 'editor': 448,\n",
       " 'public': 449,\n",
       " 'year': 450,\n",
       " 'anymore': 451,\n",
       " 'button': 452,\n",
       " 'minutes': 453,\n",
       " 'course': 454,\n",
       " 'fault': 455,\n",
       " 'nobody': 456,\n",
       " 'intelligent': 457,\n",
       " 'gives': 458,\n",
       " 'holy': 459,\n",
       " 'police': 460,\n",
       " 'child': 461,\n",
       " 'g': 462,\n",
       " 'media': 463,\n",
       " 'internet': 464,\n",
       " 'played': 465,\n",
       " 'comic': 466,\n",
       " 'between': 467,\n",
       " 'wonder': 468,\n",
       " '11': 469,\n",
       " 'hunting': 470,\n",
       " 'love': 471,\n",
       " 'die': 472,\n",
       " 'web': 473,\n",
       " 'board': 474,\n",
       " 'fine': 475,\n",
       " 'music': 476,\n",
       " 'price': 477,\n",
       " 'trouble': 478,\n",
       " 'mine': 479,\n",
       " 'apple': 480,\n",
       " 'water': 481,\n",
       " 'democrats': 482,\n",
       " 'cut': 483,\n",
       " 'cage': 484,\n",
       " 'high': 485,\n",
       " 'property': 486,\n",
       " 'killed': 487,\n",
       " 'past': 488,\n",
       " 'almost': 489,\n",
       " 'working': 490,\n",
       " 'canada': 491,\n",
       " 'law': 492,\n",
       " 'tree': 493,\n",
       " 'brilliant': 494,\n",
       " 'iraq': 495,\n",
       " '2byears': 496,\n",
       " 'picture': 497,\n",
       " 'try': 498,\n",
       " 'couple': 499,\n",
       " 'lack': 500,\n",
       " 'claim': 501,\n",
       " 'scientology': 502,\n",
       " 'hardware': 503,\n",
       " 'side': 504,\n",
       " 'title': 505,\n",
       " 'culture': 506,\n",
       " 'bill': 507,\n",
       " 'whatever': 508,\n",
       " 'data': 509,\n",
       " 'care': 510,\n",
       " 'reference': 511,\n",
       " 'forget': 512,\n",
       " 'told': 513,\n",
       " 'south': 514,\n",
       " 'ad': 515,\n",
       " 'dangerous': 516,\n",
       " 'tried': 517,\n",
       " 'mind': 518,\n",
       " 'close': 519,\n",
       " 'red': 520,\n",
       " 'environment': 521,\n",
       " 'closed': 522,\n",
       " 'handle': 523,\n",
       " 'mail': 524,\n",
       " 'thinks': 525,\n",
       " 'interesting': 526,\n",
       " 'information': 527,\n",
       " 'truck': 528,\n",
       " 'went': 529,\n",
       " 'hold': 530,\n",
       " 'dollars': 531,\n",
       " 'stories': 532,\n",
       " 'business': 533,\n",
       " 'needs': 534,\n",
       " 'continue': 535,\n",
       " 'certainly': 536,\n",
       " 'deal': 537,\n",
       " 'done': 538,\n",
       " 'ok': 539,\n",
       " 'few': 540,\n",
       " '5': 541,\n",
       " 'neither': 542,\n",
       " 'move': 543,\n",
       " 'phrase': 544,\n",
       " 'girl': 545,\n",
       " 'considering': 546,\n",
       " 'assume': 547,\n",
       " 'nah': 548,\n",
       " \"you'll\": 549,\n",
       " 'laugh': 550,\n",
       " 'personally': 551,\n",
       " 'happen': 552,\n",
       " 'religion': 553,\n",
       " 'seriously': 554,\n",
       " 'eat': 555,\n",
       " 'since': 556,\n",
       " 'learn': 557,\n",
       " 'five': 558,\n",
       " 'nearly': 559,\n",
       " 'online': 560,\n",
       " 'wow': 561,\n",
       " 'soon': 562,\n",
       " 'previous': 563,\n",
       " 'million': 564,\n",
       " 'practically': 565,\n",
       " 'ftw': 566,\n",
       " 'machines': 567,\n",
       " 'error': 568,\n",
       " 'fair': 569,\n",
       " 'quality': 570,\n",
       " 'hackers': 571,\n",
       " 'app': 572,\n",
       " 'press': 573,\n",
       " 'walmart': 574,\n",
       " 'card': 575,\n",
       " 'jesus': 576,\n",
       " 'foreign': 577,\n",
       " '000': 578,\n",
       " 'plus': 579,\n",
       " 'members': 580,\n",
       " 'others': 581,\n",
       " 'church': 582,\n",
       " 'full': 583,\n",
       " 'honestly': 584,\n",
       " 'compared': 585,\n",
       " 'available': 586,\n",
       " 'republican': 587,\n",
       " 'note': 588,\n",
       " 'bed': 589,\n",
       " 'dead': 590,\n",
       " 'implemented': 591,\n",
       " 'win': 592,\n",
       " 'sentence': 593,\n",
       " 'anything': 594,\n",
       " 'takes': 595,\n",
       " 'rational': 596,\n",
       " 'research': 597,\n",
       " 'thread': 598,\n",
       " 'php': 599,\n",
       " 'hat': 600,\n",
       " 'likes': 601,\n",
       " 'nuts': 602,\n",
       " 'save': 603,\n",
       " 'commute': 604,\n",
       " 'california': 605,\n",
       " 'ass': 606,\n",
       " 'obama': 607,\n",
       " 'race': 608,\n",
       " 'cbs': 609,\n",
       " 'unless': 610,\n",
       " 'huge': 611,\n",
       " 'illegal': 612,\n",
       " 'taking': 613,\n",
       " 'b': 614,\n",
       " 'sign': 615,\n",
       " 'looking': 616,\n",
       " 'line': 617,\n",
       " 'suppose': 618,\n",
       " 'divide': 619,\n",
       " 'growth': 620,\n",
       " 'double': 621,\n",
       " 'notice': 622,\n",
       " 'cruise': 623,\n",
       " 'president': 624,\n",
       " 'de': 625,\n",
       " 'live': 626,\n",
       " 'corporate': 627,\n",
       " 'tax': 628,\n",
       " 'dude': 629,\n",
       " 'necessarily': 630,\n",
       " 'sick': 631,\n",
       " 'biggest': 632,\n",
       " 'add': 633,\n",
       " 'security': 634,\n",
       " 'privacy': 635,\n",
       " 'ronnie': 636,\n",
       " 'fix': 637,\n",
       " 'situation': 638,\n",
       " 'effect': 639,\n",
       " 'bong': 640,\n",
       " 'whose': 641,\n",
       " 'administration': 642,\n",
       " 'american': 643,\n",
       " 'easier': 644,\n",
       " 'potential': 645,\n",
       " 'common': 646,\n",
       " 'created': 647,\n",
       " 'image': 648,\n",
       " 'emphasize': 649,\n",
       " '9': 650,\n",
       " 'giving': 651,\n",
       " 'difference': 652,\n",
       " 'bought': 653,\n",
       " \"12'\": 654,\n",
       " 'market': 655,\n",
       " 'built': 656,\n",
       " 'gallon': 657,\n",
       " 'insane': 658,\n",
       " 'nice': 659,\n",
       " '10': 660,\n",
       " 'cheaper': 661,\n",
       " 'inside': 662,\n",
       " 'bigger': 663,\n",
       " 'wanna': 664,\n",
       " 'sell': 665,\n",
       " 'sleep': 666,\n",
       " 'friend': 667,\n",
       " 'products': 668,\n",
       " 'pass': 669,\n",
       " 'laptop': 670,\n",
       " 'production': 671,\n",
       " 'fell': 672,\n",
       " 'poor': 673,\n",
       " 'entire': 674,\n",
       " 'irrational': 675,\n",
       " 'private': 676,\n",
       " 'none': 677,\n",
       " 'upmodded': 678,\n",
       " 'docid': 679,\n",
       " 'q': 680,\n",
       " 'possibly': 681,\n",
       " 'software': 682,\n",
       " 'three': 683,\n",
       " 'programming': 684,\n",
       " 'libertarian': 685,\n",
       " 'disagree': 686,\n",
       " 'explain': 687,\n",
       " 'replace': 688,\n",
       " 'torture': 689,\n",
       " 'understanding': 690,\n",
       " 'racist': 691,\n",
       " 'happens': 692,\n",
       " 'apples': 693,\n",
       " 'mr': 694,\n",
       " 'however': 695,\n",
       " 'another': 696,\n",
       " 'everybody': 697,\n",
       " 're': 698,\n",
       " 'lucky': 699,\n",
       " 'prove': 700,\n",
       " 'check': 701,\n",
       " 'hand': 702,\n",
       " 'alive': 703,\n",
       " 'county': 704,\n",
       " 'states': 705,\n",
       " 'candidate': 706,\n",
       " 'wind': 707,\n",
       " 'missed': 708,\n",
       " 'actual': 709,\n",
       " 'specific': 710,\n",
       " 'ruby': 711,\n",
       " 'home': 712,\n",
       " 'htm': 713,\n",
       " 'involve': 714,\n",
       " 'risk': 715,\n",
       " '2007': 716,\n",
       " 'happy': 717,\n",
       " 'numbers': 718,\n",
       " 'safety': 719,\n",
       " \"'it's\": 720,\n",
       " 'suck': 721,\n",
       " 'power': 722,\n",
       " 'reading': 723,\n",
       " 'tools': 724,\n",
       " 'e': 725,\n",
       " 'example': 726,\n",
       " 'black': 727,\n",
       " 'h': 728,\n",
       " 'hot': 729,\n",
       " 'fail': 730,\n",
       " 'play': 731,\n",
       " \"they've\": 732,\n",
       " 'known': 733,\n",
       " 'site': 734,\n",
       " 'started': 735,\n",
       " 'giant': 736,\n",
       " 'followers': 737,\n",
       " 'response': 738,\n",
       " 'works': 739,\n",
       " 'postgres': 740,\n",
       " 'standards': 741,\n",
       " 'set': 742,\n",
       " 'gotta': 743,\n",
       " 'false': 744,\n",
       " 'worse': 745,\n",
       " 'plastic': 746,\n",
       " 'totally': 747,\n",
       " 'dollar': 748,\n",
       " 'completely': 749,\n",
       " 'problem': 750,\n",
       " 'depending': 751,\n",
       " 'sun': 752,\n",
       " 'force': 753,\n",
       " 'doctor': 754,\n",
       " 'limited': 755,\n",
       " 'dhurka': 756,\n",
       " 'reasons': 757,\n",
       " 'state': 758,\n",
       " 'saw': 759,\n",
       " 'deliver': 760,\n",
       " 'talk': 761,\n",
       " 'highly': 762,\n",
       " 'im': 763,\n",
       " 'kid': 764,\n",
       " 'club': 765,\n",
       " 'harper': 766,\n",
       " 'opinion': 767,\n",
       " 'facts': 768,\n",
       " 'based': 769,\n",
       " 'sweden': 770,\n",
       " 'babies': 771,\n",
       " 'allow': 772,\n",
       " 'boy': 773,\n",
       " 'postgresql': 774,\n",
       " 'corner': 775,\n",
       " 'company': 776,\n",
       " 'sent': 777,\n",
       " 'straight': 778,\n",
       " 'service': 779,\n",
       " 'asking': 780,\n",
       " 'expect': 781,\n",
       " 'voice': 782,\n",
       " 'traffic': 783,\n",
       " '90': 784,\n",
       " 'quick': 785,\n",
       " 'politics': 786,\n",
       " 'correct': 787,\n",
       " 'age': 788,\n",
       " 'anti': 789,\n",
       " 'ask': 790,\n",
       " 'rejected': 791,\n",
       " 'insert': 792,\n",
       " '100': 793,\n",
       " 'context': 794,\n",
       " 'deep': 795,\n",
       " 'window': 796,\n",
       " 'sync': 797,\n",
       " 'unfortunately': 798,\n",
       " 'result': 799,\n",
       " 'fertility': 800,\n",
       " 'possible': 801,\n",
       " 'perl': 802,\n",
       " 'prefer': 803,\n",
       " 'choices': 804,\n",
       " 'duck': 805,\n",
       " 'hey': 806,\n",
       " 'pictures': 807,\n",
       " 'sort': 808,\n",
       " 'crap': 809,\n",
       " 'lie': 810,\n",
       " 'docs': 811,\n",
       " 'seeing': 812,\n",
       " 'rest': 813,\n",
       " \"'i\": 814,\n",
       " 'rain': 815,\n",
       " 'looked': 816,\n",
       " 'male': 817,\n",
       " 'zero': 818,\n",
       " 'uses': 819,\n",
       " 'learned': 820,\n",
       " 'school': 821,\n",
       " 'choose': 822,\n",
       " 'women': 823,\n",
       " 'choosing': 824,\n",
       " 'children': 825,\n",
       " 'type': 826,\n",
       " 'stake': 827,\n",
       " 'glider': 828,\n",
       " 'personal': 829,\n",
       " 'sweet': 830,\n",
       " 'genocide': 831,\n",
       " 'idea': 832,\n",
       " 'ideas': 833,\n",
       " 'vim': 834,\n",
       " 'stupid': 835,\n",
       " 'debit': 836,\n",
       " 'windows': 837,\n",
       " 'ed': 838,\n",
       " 'restroom': 839,\n",
       " 'sex': 840,\n",
       " 'nationals': 841,\n",
       " 'universities': 842,\n",
       " 'charge': 843,\n",
       " 'students': 844,\n",
       " 'taxation': 845,\n",
       " 'eu': 846,\n",
       " 'christian': 847,\n",
       " 'later': 848,\n",
       " 'worth': 849,\n",
       " 'macbook': 850,\n",
       " 'pro': 851,\n",
       " 'user': 852,\n",
       " '99': 853,\n",
       " 'carefully': 854,\n",
       " 'figures': 855,\n",
       " 'heavily': 856,\n",
       " 'hex': 857,\n",
       " 'shirt': 858,\n",
       " 'coder': 859,\n",
       " \"we'd\": 860,\n",
       " 'bashing': 861,\n",
       " 'typing': 862,\n",
       " 'action': 863,\n",
       " 'record': 864,\n",
       " 'logic': 865,\n",
       " 'due': 866,\n",
       " \"microsoft's\": 867,\n",
       " 'posted': 868,\n",
       " 'redditors': 869,\n",
       " 'twins': 870,\n",
       " 'java': 871,\n",
       " 'military': 872,\n",
       " 'argue': 873,\n",
       " 'xenu': 874,\n",
       " 'credit': 875,\n",
       " 'steel': 876,\n",
       " \"things'\": 877,\n",
       " 'near': 878,\n",
       " 'bottom': 879,\n",
       " 'systems': 880,\n",
       " \"america's\": 881,\n",
       " 'fits': 882,\n",
       " 'texas': 883,\n",
       " 'd': 884,\n",
       " 'wins': 885,\n",
       " 'protest': 886,\n",
       " 'golden': 887,\n",
       " 'local': 888,\n",
       " 'tv': 889,\n",
       " 'normally': 890,\n",
       " 'broadcast': 891,\n",
       " 'n': 892,\n",
       " 'mentioned': 893,\n",
       " 'porn': 894,\n",
       " \"'media'\": 895,\n",
       " 'movies': 896,\n",
       " 'naked': 897,\n",
       " 'covers': 898,\n",
       " 'anywhere': 899,\n",
       " 'besides': 900,\n",
       " 'judgements': 901,\n",
       " 'character': 902,\n",
       " 'kinda': 903,\n",
       " 'citation': 904,\n",
       " 'supposed': 905,\n",
       " 'glass': 906,\n",
       " 'guys': 907,\n",
       " 'follow': 908,\n",
       " 'xkcd': 909,\n",
       " 'bet': 910,\n",
       " 'speak': 911,\n",
       " 'viewpoint': 912,\n",
       " 'agreed': 913,\n",
       " 'audio': 914,\n",
       " '72': 915,\n",
       " 'estimate': 916,\n",
       " 'original': 917,\n",
       " 'amount': 918,\n",
       " '01': 919,\n",
       " 'tom': 920,\n",
       " \"weren't\": 921,\n",
       " 'scotland': 922,\n",
       " 'fucked': 923,\n",
       " 'welcome': 924,\n",
       " 'beer': 925,\n",
       " 'drinking': 926,\n",
       " 'green': 927,\n",
       " 'selling': 928,\n",
       " 'uh': 929,\n",
       " 'payer': 930,\n",
       " \"state'\": 931,\n",
       " 'upvote': 932,\n",
       " 'voting': 933,\n",
       " 'view': 934,\n",
       " 'hate': 935,\n",
       " 'sacrifice': 936,\n",
       " 'sake': 937,\n",
       " 'games': 938,\n",
       " 'harass': 939,\n",
       " 'sucks': 940,\n",
       " 'changes': 941,\n",
       " 'evil': 942,\n",
       " 'diversion': 943,\n",
       " 'february': 944,\n",
       " 'san': 945,\n",
       " 'saddam': 946,\n",
       " 'found': 947,\n",
       " 'claims': 948,\n",
       " 'report': 949,\n",
       " 'hour': 950,\n",
       " 'mistakes': 951,\n",
       " 'came': 952,\n",
       " 'definition': 953,\n",
       " 'memory': 954,\n",
       " 'losers': 955,\n",
       " 'normal': 956,\n",
       " 'liberals': 957,\n",
       " 'hilarious': 958,\n",
       " 'model': 959,\n",
       " 'dvd': 960,\n",
       " 'fluid': 961,\n",
       " 'ounces': 962,\n",
       " 'index': 963,\n",
       " 'rich': 964,\n",
       " \"kucinich's\": 965,\n",
       " 'republicans': 966,\n",
       " 'raise': 967,\n",
       " 'pounds': 968,\n",
       " 'virtual': 969,\n",
       " 'shark': 970,\n",
       " 'boat': 971,\n",
       " 'salsa': 972,\n",
       " 'agreement': 973,\n",
       " 'ways': 974,\n",
       " 'elsewhere': 975,\n",
       " 'decision': 976,\n",
       " 'paying': 977,\n",
       " 'decisions': 978,\n",
       " 'design': 979,\n",
       " 'decide': 980,\n",
       " 'killer': 981,\n",
       " 'street': 982,\n",
       " 'name': 983,\n",
       " 'crazy': 984,\n",
       " 'bastard': 985,\n",
       " 'prize': 986,\n",
       " \"they'll\": 987,\n",
       " 'future': 988,\n",
       " 'dunno': 989,\n",
       " 'closer': 990,\n",
       " 'quarter': 991,\n",
       " 'scared': 992,\n",
       " 'break': 993,\n",
       " 'apply': 994,\n",
       " 'admire': 995,\n",
       " 'fan': 996,\n",
       " 'lives': 997,\n",
       " 'anybody': 998,\n",
       " 'zoom': 999,\n",
       " 'program': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Targetindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train From File:\n",
      "\n",
      "Found 1000 sentences.\n",
      "Found 1000 sequences.\n",
      "Found 4129 unique tokens.\n",
      "Found 4129 unique words.\n"
     ]
    }
   ],
   "source": [
    "trainInputDecoded = to_categorical([sequencesInputDecode], num_classes=nDecoderToken)\n",
    "trainTargetDecoded = to_categorical([sequencesTargetDecode], num_classes=nDecoderToken)\n",
    "\n",
    "trainInputDecoded = trainInputDecoded.reshape(len(trainTarget), max_len, nDecoderToken)\n",
    "trainTargetDecoded = trainTargetDecoded.reshape(len(trainTarget), max_len, nDecoderToken)\n",
    "\n",
    "print(\"Train From File:\\n\")\n",
    "print('Found %s sentences.' %len(trainTarget))\n",
    "print('Found %s sequences.' %len(sequencesInputDecode))\n",
    "print('Found %s unique tokens.' % len(Targetindex))\n",
    "print('Found %s unique words.' % len(Targetcount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 100, 4130)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainInputDecoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "nUnits = 300\n",
    "# Define training encoder\n",
    "encoder_inputs = Input(shape=(1000, nEncoderToken))\n",
    "encoder = LSTM(nUnits, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Define training decoder\n",
    "decoder_inputs = Input(shape=(None, nDecoderToken))\n",
    "decoder_lstm = LSTM(nUnits, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "decoder_dense = Dense(nDecoderToken, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1000/1000 [==============================] - 197s 197ms/step - loss: 3.6845 - acc: 0.8187\n",
      "Epoch 2/2\n",
      "1000/1000 [==============================] - 210s 210ms/step - loss: 1.5344 - acc: 0.8456\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1cfce99b308>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit([trainInputEncoded, trainInputDecoded], trainTargetDecoded, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse-lookup token index to decode sequences back to something readable.\n",
    "reverse_input_char_index = dict((i, char) for char, i in Inputindex.items())\n",
    "reverse_target_char_index = dict((i, char) for char, i in Targetindex.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define inference encoder\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "# Define inference decoder\n",
    "decoder_state_input_h = Input(shape=(nUnits,))\n",
    "decoder_state_input_c = Input(shape=(nUnits,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'encoder_input_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-8e8f0df63473>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;31m# Take one sequence (part of the training set)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;31m# for trying out decoding.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0minput_seq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder_input_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mseq_index\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mseq_index\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mdecoded_sentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecode_sequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_seq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'encoder_input_data' is not defined"
     ]
    }
   ],
   "source": [
    "for seq_index in range(100):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'states_value' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-44552edb873d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdecoded_sentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0moutput_tokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecoder_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtarget_seq\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstates_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# Sample a token\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'states_value' is not defined"
     ]
    }
   ],
   "source": [
    "# Encode the input as state vectors.\n",
    "states_value = encoder_model.predict(input_seq)\n",
    "# Generate empty target sequence of length 1.\n",
    "target_seq = np.zeros((1, 1, nDecoderToken))\n",
    "\n",
    "decoded_sentence = ''\n",
    "output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "# Sample a token\n",
    "sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "decoded_sentence += sampled_char\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'input_10:0' shape=(?, 1000, 4305) dtype=float32>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "When feeding symbolic tensors to a model, we expect thetensors to have a static batch size. Got tensor with shape: (None, None, 4305)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-97b9215e2ae7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[1;31m# for trying out decoding.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[0minput_seq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder_inputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mseq_index\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mseq_index\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m     \u001b[0mdecoded_sentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecode_sequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_seq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'-'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Input sentence:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_texts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mseq_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-43-97b9215e2ae7>\u001b[0m in \u001b[0;36mdecode_sequence\u001b[1;34m(input_seq)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdecode_sequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_seq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;31m# Encode the input as state vectors.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mstates_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_seq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m# Generate empty target sequence of length 1.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[0;32m   1147\u001b[0m                              'argument.')\n\u001b[0;32m   1148\u001b[0m         \u001b[1;31m# Validate user data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1149\u001b[1;33m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1150\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 751\u001b[1;33m             exception_prefix='input')\n\u001b[0m\u001b[0;32m    752\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    753\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'DataFrame'\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mstandardize_single_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'DataFrame'\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mstandardize_single_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_single_array\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     23\u001b[0m                 \u001b[1;34m'When feeding symbolic tensors to a model, we expect the'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m                 \u001b[1;34m'tensors to have a static batch size. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m                 'Got tensor with shape: %s' % str(shape))\n\u001b[0m\u001b[0;32m     26\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: When feeding symbolic tensors to a model, we expect thetensors to have a static batch size. Got tensor with shape: (None, None, 4305)"
     ]
    }
   ],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, nDecoderToken))\n",
    "    \n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_len):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, nDecoderToken))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence\n",
    "\n",
    "\n",
    "for seq_index in range(100):\n",
    "    # Take one sequence (part of the training set) for trying out decoding.\n",
    "    input_seq = encoder_inputs[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate empty target sequence of length 1.\n",
    "target_seq = np.zeros((1, 1, nDecoderToken))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 4130)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = np.zeros((1, 100, nEncoderToken))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = trainInputEncoded[20:30, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 100, 4305)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputSource = np.zeros((5, 100, nEncoderToken))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 100, 4305)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputSource.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[8.8839579e-01 9.5118629e-03 6.7120218e-03 ... 3.5038986e-06\n",
      "   3.5079911e-06 3.5739542e-06]]]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "state = encoder_model.predict(source)\n",
    "yhat, h, c = decoder_model.predict([target_seq] + state)\n",
    "\n",
    "sampled_token_index = np.argmax(yhat)\n",
    "\n",
    "print(yhat)\n",
    "print(sampled_token_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (4130) into shape (100,4305)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-88-8771f681f143>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0myhat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecoder_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtarget_seq\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;31m# Store prediction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0moutputSource\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0myhat\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myhat\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;31m# Update state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not broadcast input array from shape (4130) into shape (100,4305)"
     ]
    }
   ],
   "source": [
    "nSteps = 5\n",
    "state = encoder_model.predict(source)\n",
    "# Collect predictions\n",
    "output = list()\n",
    "for t in range(nSteps):\n",
    "    # Predict next word\n",
    "    yhat, h, c = decoder_model.predict([target_seq] + state)\n",
    "    # Store prediction\n",
    "    outputSource[t, :, :] = yhat[0,0,:]\n",
    "    output.append(yhat[0,0,:])\n",
    "    # Update state\n",
    "    state = [h, c]\n",
    "    # Update target sequence\n",
    "    target_seq = yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 4130)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-85-961ed67232b2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample a token\n",
    "sampled_token_index = np.argmax(target_seq[0, -1, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_token_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "decoded_sentence += sampled_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'the',\n",
       " 2: 'newlinechar',\n",
       " 3: 'to',\n",
       " 4: 'a',\n",
       " 5: 'i',\n",
       " 6: 'you',\n",
       " 7: 'and',\n",
       " 8: 'of',\n",
       " 9: 'is',\n",
       " 10: 'that',\n",
       " 11: 'it',\n",
       " 12: 'in',\n",
       " 13: 'for',\n",
       " 14: 'on',\n",
       " 15: 'not',\n",
       " 16: 'they',\n",
       " 17: 'be',\n",
       " 18: 'have',\n",
       " 19: 'was',\n",
       " 20: 'are',\n",
       " 21: 'but',\n",
       " 22: 'if',\n",
       " 23: 'with',\n",
       " 24: \"it's\",\n",
       " 25: 'just',\n",
       " 26: 'my',\n",
       " 27: 'your',\n",
       " 28: 'or',\n",
       " 29: 'like',\n",
       " 30: 'as',\n",
       " 31: 'would',\n",
       " 32: 'all',\n",
       " 33: 'what',\n",
       " 34: 'this',\n",
       " 35: \"don't\",\n",
       " 36: 'so',\n",
       " 37: 'he',\n",
       " 38: 'http',\n",
       " 39: 'think',\n",
       " 40: 'people',\n",
       " 41: 'do',\n",
       " 42: 'me',\n",
       " 43: 'no',\n",
       " 44: 'at',\n",
       " 45: \"'\",\n",
       " 46: 'from',\n",
       " 47: 'one',\n",
       " 48: 'can',\n",
       " 49: 'gt',\n",
       " 50: 'how',\n",
       " 51: 'there',\n",
       " 52: 'about',\n",
       " 53: 'more',\n",
       " 54: 'we',\n",
       " 55: 'get',\n",
       " 56: 'know',\n",
       " 57: 'com',\n",
       " 58: 'some',\n",
       " 59: 'them',\n",
       " 60: 'then',\n",
       " 61: 'an',\n",
       " 62: \"that's\",\n",
       " 63: 'will',\n",
       " 64: 'up',\n",
       " 65: \"i'm\",\n",
       " 66: 'good',\n",
       " 67: 'well',\n",
       " 68: 'by',\n",
       " 69: 'who',\n",
       " 70: 'because',\n",
       " 71: 'make',\n",
       " 72: 'than',\n",
       " 73: 'their',\n",
       " 74: 'his',\n",
       " 75: 'too',\n",
       " 76: 'when',\n",
       " 77: 'www',\n",
       " 78: 'out',\n",
       " 79: 'why',\n",
       " 80: 'actually',\n",
       " 81: 'which',\n",
       " 82: 'reddit',\n",
       " 83: 'has',\n",
       " 84: 'should',\n",
       " 85: 'point',\n",
       " 86: 'only',\n",
       " 87: 'back',\n",
       " 88: 'time',\n",
       " 89: 'did',\n",
       " 90: 'see',\n",
       " 91: 'were',\n",
       " 92: 'go',\n",
       " 93: 'even',\n",
       " 94: 'still',\n",
       " 95: 'most',\n",
       " 96: 'now',\n",
       " 97: 'man',\n",
       " 98: \"you're\",\n",
       " 99: 'much',\n",
       " 100: 'could',\n",
       " 101: 'those',\n",
       " 102: \"didn't\",\n",
       " 103: 'other',\n",
       " 104: 'really',\n",
       " 105: 'mean',\n",
       " 106: 'used',\n",
       " 107: \"can't\",\n",
       " 108: \"doesn't\",\n",
       " 109: 'maybe',\n",
       " 110: 'any',\n",
       " 111: 'us',\n",
       " 112: 'first',\n",
       " 113: 'yes',\n",
       " 114: 'thing',\n",
       " 115: 'want',\n",
       " 116: 'been',\n",
       " 117: 'here',\n",
       " 118: 'does',\n",
       " 119: \"he's\",\n",
       " 120: 'yeah',\n",
       " 121: 'amp',\n",
       " 122: 'being',\n",
       " 123: 'right',\n",
       " 124: 'off',\n",
       " 125: 'comment',\n",
       " 126: 'someone',\n",
       " 127: 'got',\n",
       " 128: 'sure',\n",
       " 129: 'again',\n",
       " 130: 'read',\n",
       " 131: '2',\n",
       " 132: 'better',\n",
       " 133: 'down',\n",
       " 134: 'over',\n",
       " 135: 'many',\n",
       " 136: 'into',\n",
       " 137: 'true',\n",
       " 138: 'very',\n",
       " 139: 'day',\n",
       " 140: 'saying',\n",
       " 141: 'said',\n",
       " 142: 'watch',\n",
       " 143: 'thought',\n",
       " 144: 'guy',\n",
       " 145: 'had',\n",
       " 146: 'way',\n",
       " 147: 'everyone',\n",
       " 148: 'probably',\n",
       " 149: 'pretty',\n",
       " 150: 'our',\n",
       " 151: 'god',\n",
       " 152: 'sorry',\n",
       " 153: 'put',\n",
       " 154: 'where',\n",
       " 155: '1',\n",
       " 156: 'having',\n",
       " 157: 'youtube',\n",
       " 158: 'say',\n",
       " 159: 'thanks',\n",
       " 160: \"isn't\",\n",
       " 161: 'paul',\n",
       " 162: 'believe',\n",
       " 163: 'money',\n",
       " 164: 'call',\n",
       " 165: 'already',\n",
       " 166: 'oh',\n",
       " 167: 'least',\n",
       " 168: 'agree',\n",
       " 169: 'use',\n",
       " 170: 'without',\n",
       " 171: 'long',\n",
       " 172: 'same',\n",
       " 173: 'org',\n",
       " 174: 'two',\n",
       " 175: 'its',\n",
       " 176: 'free',\n",
       " 177: 'ever',\n",
       " 178: 'also',\n",
       " 179: 'tell',\n",
       " 180: 'life',\n",
       " 181: \"i'd\",\n",
       " 182: 'anyone',\n",
       " 183: 'take',\n",
       " 184: 'great',\n",
       " 185: 'might',\n",
       " 186: 'find',\n",
       " 187: 'always',\n",
       " 188: 'bad',\n",
       " 189: 'enough',\n",
       " 190: 'nothing',\n",
       " 191: 'though',\n",
       " 192: 'something',\n",
       " 193: 'these',\n",
       " 194: 'world',\n",
       " 195: 'give',\n",
       " 196: 'around',\n",
       " 197: 'system',\n",
       " 198: 'never',\n",
       " 199: \"they're\",\n",
       " 200: 'every',\n",
       " 201: 'look',\n",
       " 202: \"wasn't\",\n",
       " 203: 'little',\n",
       " 204: 'against',\n",
       " 205: 'things',\n",
       " 206: 'after',\n",
       " 207: 'guess',\n",
       " 208: 'o',\n",
       " 209: \"i've\",\n",
       " 210: 'government',\n",
       " 211: 'wait',\n",
       " 212: 'v',\n",
       " 213: 'while',\n",
       " 214: 'may',\n",
       " 215: 'code',\n",
       " 216: 'quite',\n",
       " 217: 'case',\n",
       " 218: 'exactly',\n",
       " 219: 'own',\n",
       " 220: 'made',\n",
       " 221: 'yet',\n",
       " 222: 'using',\n",
       " 223: 'reason',\n",
       " 224: 'him',\n",
       " 225: 'real',\n",
       " 226: 'job',\n",
       " 227: 'need',\n",
       " 228: 'seems',\n",
       " 229: 'such',\n",
       " 230: 'html',\n",
       " 231: 'wikipedia',\n",
       " 232: 'come',\n",
       " 233: 'work',\n",
       " 234: 'going',\n",
       " 235: 'norway',\n",
       " 236: 'wrong',\n",
       " 237: 'x',\n",
       " 238: \"there's\",\n",
       " 239: 'please',\n",
       " 240: 'makes',\n",
       " 241: 'doing',\n",
       " 242: 'both',\n",
       " 243: 'before',\n",
       " 244: 'buy',\n",
       " 245: 'write',\n",
       " 246: \"what's\",\n",
       " 247: 'kind',\n",
       " 248: 'part',\n",
       " 249: 'yourself',\n",
       " 250: 'stuff',\n",
       " 251: 'seen',\n",
       " 252: 'new',\n",
       " 253: 'bush',\n",
       " 254: 'starting',\n",
       " 255: 'years',\n",
       " 256: 'different',\n",
       " 257: 'once',\n",
       " 258: 'source',\n",
       " 259: 'fucking',\n",
       " 260: 'comments',\n",
       " 261: 'lot',\n",
       " 262: 'talking',\n",
       " 263: 'else',\n",
       " 264: 'under',\n",
       " 265: 'mysql',\n",
       " 266: 'likely',\n",
       " 267: 'oil',\n",
       " 268: 'article',\n",
       " 269: 'votes',\n",
       " 270: 'far',\n",
       " 271: 'books',\n",
       " 272: 'hope',\n",
       " 273: 'pay',\n",
       " 274: 'until',\n",
       " 275: 'order',\n",
       " 276: 'away',\n",
       " 277: 'she',\n",
       " 278: 'less',\n",
       " 279: 'through',\n",
       " 280: 'head',\n",
       " 281: 'trying',\n",
       " 282: 'freedom',\n",
       " 283: \"i'll\",\n",
       " 284: 'am',\n",
       " 285: 'video',\n",
       " 286: 'hard',\n",
       " 287: 's',\n",
       " 288: 'means',\n",
       " 289: 'story',\n",
       " 290: 'last',\n",
       " 291: 'ah',\n",
       " 292: \"we're\",\n",
       " 293: 'meant',\n",
       " 294: 'getting',\n",
       " 295: 'question',\n",
       " 296: 'argument',\n",
       " 297: 'old',\n",
       " 298: 'choice',\n",
       " 299: \"wouldn't\",\n",
       " 300: 'her',\n",
       " 301: 'fact',\n",
       " 302: 'fuck',\n",
       " 303: 'en',\n",
       " 304: 'wiki',\n",
       " 305: 'perhaps',\n",
       " 306: 'help',\n",
       " 307: 'clear',\n",
       " 308: 'keep',\n",
       " 309: 'joke',\n",
       " 310: 'understand',\n",
       " 311: 'kill',\n",
       " 312: 'car',\n",
       " 313: 'linux',\n",
       " 314: 'funny',\n",
       " 315: 'called',\n",
       " 316: 'instead',\n",
       " 317: 'support',\n",
       " 318: 'imagine',\n",
       " 319: \"you've\",\n",
       " 320: 'hell',\n",
       " 321: 'indeed',\n",
       " 322: 'book',\n",
       " 323: 'show',\n",
       " 324: 'wish',\n",
       " 325: 'left',\n",
       " 326: 'everything',\n",
       " 327: 'remember',\n",
       " 328: 'stop',\n",
       " 329: 'awesome',\n",
       " 330: 'lol',\n",
       " 331: 'lt',\n",
       " 332: 'literally',\n",
       " 333: 'days',\n",
       " 334: 'serious',\n",
       " 335: 'rather',\n",
       " 336: 'goes',\n",
       " 337: 'house',\n",
       " 338: 'making',\n",
       " 339: 'bit',\n",
       " 340: 'google',\n",
       " 341: '3d0',\n",
       " 342: 'change',\n",
       " 343: 'feel',\n",
       " 344: 'content',\n",
       " 345: 'p',\n",
       " 346: 'sense',\n",
       " 347: 'next',\n",
       " 348: 'gets',\n",
       " 349: 'roll',\n",
       " 350: \"'the\",\n",
       " 351: 'damn',\n",
       " 352: 'link',\n",
       " 353: 'thank',\n",
       " 354: 'net',\n",
       " 355: 'language',\n",
       " 356: 'thinking',\n",
       " 357: 'fun',\n",
       " 358: 'points',\n",
       " 359: 'thats',\n",
       " 360: 'consider',\n",
       " 361: 'c',\n",
       " 362: 'info',\n",
       " 363: 'comes',\n",
       " 364: 'ron',\n",
       " 365: 'place',\n",
       " 366: 'whole',\n",
       " 367: 'able',\n",
       " 368: 'big',\n",
       " 369: 'small',\n",
       " 370: 'pizza',\n",
       " 371: 'shit',\n",
       " 372: 'cool',\n",
       " 373: 'must',\n",
       " 374: 'game',\n",
       " 375: 'vi',\n",
       " 376: 'looks',\n",
       " 377: 'uk',\n",
       " 378: 'non',\n",
       " 379: 'start',\n",
       " 380: 'history',\n",
       " 381: 'knows',\n",
       " 382: 'second',\n",
       " 383: 'click',\n",
       " 384: 'says',\n",
       " 385: 'mention',\n",
       " 386: 'obviously',\n",
       " 387: 'top',\n",
       " 388: 'each',\n",
       " 389: 'anyway',\n",
       " 390: 'sir',\n",
       " 391: 'yep',\n",
       " 392: 'experience',\n",
       " 393: 'end',\n",
       " 394: 'mccain',\n",
       " 395: 'national',\n",
       " 396: 'news',\n",
       " 397: \"haven't\",\n",
       " 398: 'page',\n",
       " 399: 'version',\n",
       " 400: 'rule',\n",
       " 401: 'running',\n",
       " 402: 'friends',\n",
       " 403: 'hit',\n",
       " 404: 'myself',\n",
       " 405: 'america',\n",
       " 406: 'whether',\n",
       " 407: \"he'll\",\n",
       " 408: '3',\n",
       " 409: 'machine',\n",
       " 410: 'seem',\n",
       " 411: 'wanted',\n",
       " 412: 'country',\n",
       " 413: 'sheeple',\n",
       " 414: 'gonna',\n",
       " 415: 'except',\n",
       " 416: 'voted',\n",
       " 417: 'open',\n",
       " 418: 'heard',\n",
       " 419: 'basically',\n",
       " 420: 'parents',\n",
       " 421: 'run',\n",
       " 422: 'best',\n",
       " 423: \"won't\",\n",
       " 424: \"aren't\",\n",
       " 425: 'sounds',\n",
       " 426: 'word',\n",
       " 427: 'election',\n",
       " 428: 'gay',\n",
       " 429: 'behind',\n",
       " 430: 'jpg',\n",
       " 431: \"here's\",\n",
       " 432: 'post',\n",
       " 433: 'took',\n",
       " 434: '4',\n",
       " 435: 'etc',\n",
       " 436: '20',\n",
       " 437: 'definitely',\n",
       " 438: 'either',\n",
       " 439: 'face',\n",
       " 440: 'person',\n",
       " 441: \"couldn't\",\n",
       " 442: 'edit',\n",
       " 443: \"you'd\",\n",
       " 444: 'air',\n",
       " 445: \"let's\",\n",
       " 446: 'vote',\n",
       " 447: 'count',\n",
       " 448: 'editor',\n",
       " 449: 'public',\n",
       " 450: 'year',\n",
       " 451: 'anymore',\n",
       " 452: 'button',\n",
       " 453: 'minutes',\n",
       " 454: 'course',\n",
       " 455: 'fault',\n",
       " 456: 'nobody',\n",
       " 457: 'intelligent',\n",
       " 458: 'gives',\n",
       " 459: 'holy',\n",
       " 460: 'police',\n",
       " 461: 'child',\n",
       " 462: 'g',\n",
       " 463: 'media',\n",
       " 464: 'internet',\n",
       " 465: 'played',\n",
       " 466: 'comic',\n",
       " 467: 'between',\n",
       " 468: 'wonder',\n",
       " 469: '11',\n",
       " 470: 'hunting',\n",
       " 471: 'love',\n",
       " 472: 'die',\n",
       " 473: 'web',\n",
       " 474: 'board',\n",
       " 475: 'fine',\n",
       " 476: 'music',\n",
       " 477: 'price',\n",
       " 478: 'trouble',\n",
       " 479: 'mine',\n",
       " 480: 'apple',\n",
       " 481: 'water',\n",
       " 482: 'democrats',\n",
       " 483: 'cut',\n",
       " 484: 'cage',\n",
       " 485: 'high',\n",
       " 486: 'property',\n",
       " 487: 'killed',\n",
       " 488: 'past',\n",
       " 489: 'almost',\n",
       " 490: 'working',\n",
       " 491: 'canada',\n",
       " 492: 'law',\n",
       " 493: 'tree',\n",
       " 494: 'brilliant',\n",
       " 495: 'iraq',\n",
       " 496: '2byears',\n",
       " 497: 'picture',\n",
       " 498: 'try',\n",
       " 499: 'couple',\n",
       " 500: 'lack',\n",
       " 501: 'claim',\n",
       " 502: 'scientology',\n",
       " 503: 'hardware',\n",
       " 504: 'side',\n",
       " 505: 'title',\n",
       " 506: 'culture',\n",
       " 507: 'bill',\n",
       " 508: 'whatever',\n",
       " 509: 'data',\n",
       " 510: 'care',\n",
       " 511: 'reference',\n",
       " 512: 'forget',\n",
       " 513: 'told',\n",
       " 514: 'south',\n",
       " 515: 'ad',\n",
       " 516: 'dangerous',\n",
       " 517: 'tried',\n",
       " 518: 'mind',\n",
       " 519: 'close',\n",
       " 520: 'red',\n",
       " 521: 'environment',\n",
       " 522: 'closed',\n",
       " 523: 'handle',\n",
       " 524: 'mail',\n",
       " 525: 'thinks',\n",
       " 526: 'interesting',\n",
       " 527: 'information',\n",
       " 528: 'truck',\n",
       " 529: 'went',\n",
       " 530: 'hold',\n",
       " 531: 'dollars',\n",
       " 532: 'stories',\n",
       " 533: 'business',\n",
       " 534: 'needs',\n",
       " 535: 'continue',\n",
       " 536: 'certainly',\n",
       " 537: 'deal',\n",
       " 538: 'done',\n",
       " 539: 'ok',\n",
       " 540: 'few',\n",
       " 541: '5',\n",
       " 542: 'neither',\n",
       " 543: 'move',\n",
       " 544: 'phrase',\n",
       " 545: 'girl',\n",
       " 546: 'considering',\n",
       " 547: 'assume',\n",
       " 548: 'nah',\n",
       " 549: \"you'll\",\n",
       " 550: 'laugh',\n",
       " 551: 'personally',\n",
       " 552: 'happen',\n",
       " 553: 'religion',\n",
       " 554: 'seriously',\n",
       " 555: 'eat',\n",
       " 556: 'since',\n",
       " 557: 'learn',\n",
       " 558: 'five',\n",
       " 559: 'nearly',\n",
       " 560: 'online',\n",
       " 561: 'wow',\n",
       " 562: 'soon',\n",
       " 563: 'previous',\n",
       " 564: 'million',\n",
       " 565: 'practically',\n",
       " 566: 'ftw',\n",
       " 567: 'machines',\n",
       " 568: 'error',\n",
       " 569: 'fair',\n",
       " 570: 'quality',\n",
       " 571: 'hackers',\n",
       " 572: 'app',\n",
       " 573: 'press',\n",
       " 574: 'walmart',\n",
       " 575: 'card',\n",
       " 576: 'jesus',\n",
       " 577: 'foreign',\n",
       " 578: '000',\n",
       " 579: 'plus',\n",
       " 580: 'members',\n",
       " 581: 'others',\n",
       " 582: 'church',\n",
       " 583: 'full',\n",
       " 584: 'honestly',\n",
       " 585: 'compared',\n",
       " 586: 'available',\n",
       " 587: 'republican',\n",
       " 588: 'note',\n",
       " 589: 'bed',\n",
       " 590: 'dead',\n",
       " 591: 'implemented',\n",
       " 592: 'win',\n",
       " 593: 'sentence',\n",
       " 594: 'anything',\n",
       " 595: 'takes',\n",
       " 596: 'rational',\n",
       " 597: 'research',\n",
       " 598: 'thread',\n",
       " 599: 'php',\n",
       " 600: 'hat',\n",
       " 601: 'likes',\n",
       " 602: 'nuts',\n",
       " 603: 'save',\n",
       " 604: 'commute',\n",
       " 605: 'california',\n",
       " 606: 'ass',\n",
       " 607: 'obama',\n",
       " 608: 'race',\n",
       " 609: 'cbs',\n",
       " 610: 'unless',\n",
       " 611: 'huge',\n",
       " 612: 'illegal',\n",
       " 613: 'taking',\n",
       " 614: 'b',\n",
       " 615: 'sign',\n",
       " 616: 'looking',\n",
       " 617: 'line',\n",
       " 618: 'suppose',\n",
       " 619: 'divide',\n",
       " 620: 'growth',\n",
       " 621: 'double',\n",
       " 622: 'notice',\n",
       " 623: 'cruise',\n",
       " 624: 'president',\n",
       " 625: 'de',\n",
       " 626: 'live',\n",
       " 627: 'corporate',\n",
       " 628: 'tax',\n",
       " 629: 'dude',\n",
       " 630: 'necessarily',\n",
       " 631: 'sick',\n",
       " 632: 'biggest',\n",
       " 633: 'add',\n",
       " 634: 'security',\n",
       " 635: 'privacy',\n",
       " 636: 'ronnie',\n",
       " 637: 'fix',\n",
       " 638: 'situation',\n",
       " 639: 'effect',\n",
       " 640: 'bong',\n",
       " 641: 'whose',\n",
       " 642: 'administration',\n",
       " 643: 'american',\n",
       " 644: 'easier',\n",
       " 645: 'potential',\n",
       " 646: 'common',\n",
       " 647: 'created',\n",
       " 648: 'image',\n",
       " 649: 'emphasize',\n",
       " 650: '9',\n",
       " 651: 'giving',\n",
       " 652: 'difference',\n",
       " 653: 'bought',\n",
       " 654: \"12'\",\n",
       " 655: 'market',\n",
       " 656: 'built',\n",
       " 657: 'gallon',\n",
       " 658: 'insane',\n",
       " 659: 'nice',\n",
       " 660: '10',\n",
       " 661: 'cheaper',\n",
       " 662: 'inside',\n",
       " 663: 'bigger',\n",
       " 664: 'wanna',\n",
       " 665: 'sell',\n",
       " 666: 'sleep',\n",
       " 667: 'friend',\n",
       " 668: 'products',\n",
       " 669: 'pass',\n",
       " 670: 'laptop',\n",
       " 671: 'production',\n",
       " 672: 'fell',\n",
       " 673: 'poor',\n",
       " 674: 'entire',\n",
       " 675: 'irrational',\n",
       " 676: 'private',\n",
       " 677: 'none',\n",
       " 678: 'upmodded',\n",
       " 679: 'docid',\n",
       " 680: 'q',\n",
       " 681: 'possibly',\n",
       " 682: 'software',\n",
       " 683: 'three',\n",
       " 684: 'programming',\n",
       " 685: 'libertarian',\n",
       " 686: 'disagree',\n",
       " 687: 'explain',\n",
       " 688: 'replace',\n",
       " 689: 'torture',\n",
       " 690: 'understanding',\n",
       " 691: 'racist',\n",
       " 692: 'happens',\n",
       " 693: 'apples',\n",
       " 694: 'mr',\n",
       " 695: 'however',\n",
       " 696: 'another',\n",
       " 697: 'everybody',\n",
       " 698: 're',\n",
       " 699: 'lucky',\n",
       " 700: 'prove',\n",
       " 701: 'check',\n",
       " 702: 'hand',\n",
       " 703: 'alive',\n",
       " 704: 'county',\n",
       " 705: 'states',\n",
       " 706: 'candidate',\n",
       " 707: 'wind',\n",
       " 708: 'missed',\n",
       " 709: 'actual',\n",
       " 710: 'specific',\n",
       " 711: 'ruby',\n",
       " 712: 'home',\n",
       " 713: 'htm',\n",
       " 714: 'involve',\n",
       " 715: 'risk',\n",
       " 716: '2007',\n",
       " 717: 'happy',\n",
       " 718: 'numbers',\n",
       " 719: 'safety',\n",
       " 720: \"'it's\",\n",
       " 721: 'suck',\n",
       " 722: 'power',\n",
       " 723: 'reading',\n",
       " 724: 'tools',\n",
       " 725: 'e',\n",
       " 726: 'example',\n",
       " 727: 'black',\n",
       " 728: 'h',\n",
       " 729: 'hot',\n",
       " 730: 'fail',\n",
       " 731: 'play',\n",
       " 732: \"they've\",\n",
       " 733: 'known',\n",
       " 734: 'site',\n",
       " 735: 'started',\n",
       " 736: 'giant',\n",
       " 737: 'followers',\n",
       " 738: 'response',\n",
       " 739: 'works',\n",
       " 740: 'postgres',\n",
       " 741: 'standards',\n",
       " 742: 'set',\n",
       " 743: 'gotta',\n",
       " 744: 'false',\n",
       " 745: 'worse',\n",
       " 746: 'plastic',\n",
       " 747: 'totally',\n",
       " 748: 'dollar',\n",
       " 749: 'completely',\n",
       " 750: 'problem',\n",
       " 751: 'depending',\n",
       " 752: 'sun',\n",
       " 753: 'force',\n",
       " 754: 'doctor',\n",
       " 755: 'limited',\n",
       " 756: 'dhurka',\n",
       " 757: 'reasons',\n",
       " 758: 'state',\n",
       " 759: 'saw',\n",
       " 760: 'deliver',\n",
       " 761: 'talk',\n",
       " 762: 'highly',\n",
       " 763: 'im',\n",
       " 764: 'kid',\n",
       " 765: 'club',\n",
       " 766: 'harper',\n",
       " 767: 'opinion',\n",
       " 768: 'facts',\n",
       " 769: 'based',\n",
       " 770: 'sweden',\n",
       " 771: 'babies',\n",
       " 772: 'allow',\n",
       " 773: 'boy',\n",
       " 774: 'postgresql',\n",
       " 775: 'corner',\n",
       " 776: 'company',\n",
       " 777: 'sent',\n",
       " 778: 'straight',\n",
       " 779: 'service',\n",
       " 780: 'asking',\n",
       " 781: 'expect',\n",
       " 782: 'voice',\n",
       " 783: 'traffic',\n",
       " 784: '90',\n",
       " 785: 'quick',\n",
       " 786: 'politics',\n",
       " 787: 'correct',\n",
       " 788: 'age',\n",
       " 789: 'anti',\n",
       " 790: 'ask',\n",
       " 791: 'rejected',\n",
       " 792: 'insert',\n",
       " 793: '100',\n",
       " 794: 'context',\n",
       " 795: 'deep',\n",
       " 796: 'window',\n",
       " 797: 'sync',\n",
       " 798: 'unfortunately',\n",
       " 799: 'result',\n",
       " 800: 'fertility',\n",
       " 801: 'possible',\n",
       " 802: 'perl',\n",
       " 803: 'prefer',\n",
       " 804: 'choices',\n",
       " 805: 'duck',\n",
       " 806: 'hey',\n",
       " 807: 'pictures',\n",
       " 808: 'sort',\n",
       " 809: 'crap',\n",
       " 810: 'lie',\n",
       " 811: 'docs',\n",
       " 812: 'seeing',\n",
       " 813: 'rest',\n",
       " 814: \"'i\",\n",
       " 815: 'rain',\n",
       " 816: 'looked',\n",
       " 817: 'male',\n",
       " 818: 'zero',\n",
       " 819: 'uses',\n",
       " 820: 'learned',\n",
       " 821: 'school',\n",
       " 822: 'choose',\n",
       " 823: 'women',\n",
       " 824: 'choosing',\n",
       " 825: 'children',\n",
       " 826: 'type',\n",
       " 827: 'stake',\n",
       " 828: 'glider',\n",
       " 829: 'personal',\n",
       " 830: 'sweet',\n",
       " 831: 'genocide',\n",
       " 832: 'idea',\n",
       " 833: 'ideas',\n",
       " 834: 'vim',\n",
       " 835: 'stupid',\n",
       " 836: 'debit',\n",
       " 837: 'windows',\n",
       " 838: 'ed',\n",
       " 839: 'restroom',\n",
       " 840: 'sex',\n",
       " 841: 'nationals',\n",
       " 842: 'universities',\n",
       " 843: 'charge',\n",
       " 844: 'students',\n",
       " 845: 'taxation',\n",
       " 846: 'eu',\n",
       " 847: 'christian',\n",
       " 848: 'later',\n",
       " 849: 'worth',\n",
       " 850: 'macbook',\n",
       " 851: 'pro',\n",
       " 852: 'user',\n",
       " 853: '99',\n",
       " 854: 'carefully',\n",
       " 855: 'figures',\n",
       " 856: 'heavily',\n",
       " 857: 'hex',\n",
       " 858: 'shirt',\n",
       " 859: 'coder',\n",
       " 860: \"we'd\",\n",
       " 861: 'bashing',\n",
       " 862: 'typing',\n",
       " 863: 'action',\n",
       " 864: 'record',\n",
       " 865: 'logic',\n",
       " 866: 'due',\n",
       " 867: \"microsoft's\",\n",
       " 868: 'posted',\n",
       " 869: 'redditors',\n",
       " 870: 'twins',\n",
       " 871: 'java',\n",
       " 872: 'military',\n",
       " 873: 'argue',\n",
       " 874: 'xenu',\n",
       " 875: 'credit',\n",
       " 876: 'steel',\n",
       " 877: \"things'\",\n",
       " 878: 'near',\n",
       " 879: 'bottom',\n",
       " 880: 'systems',\n",
       " 881: \"america's\",\n",
       " 882: 'fits',\n",
       " 883: 'texas',\n",
       " 884: 'd',\n",
       " 885: 'wins',\n",
       " 886: 'protest',\n",
       " 887: 'golden',\n",
       " 888: 'local',\n",
       " 889: 'tv',\n",
       " 890: 'normally',\n",
       " 891: 'broadcast',\n",
       " 892: 'n',\n",
       " 893: 'mentioned',\n",
       " 894: 'porn',\n",
       " 895: \"'media'\",\n",
       " 896: 'movies',\n",
       " 897: 'naked',\n",
       " 898: 'covers',\n",
       " 899: 'anywhere',\n",
       " 900: 'besides',\n",
       " 901: 'judgements',\n",
       " 902: 'character',\n",
       " 903: 'kinda',\n",
       " 904: 'citation',\n",
       " 905: 'supposed',\n",
       " 906: 'glass',\n",
       " 907: 'guys',\n",
       " 908: 'follow',\n",
       " 909: 'xkcd',\n",
       " 910: 'bet',\n",
       " 911: 'speak',\n",
       " 912: 'viewpoint',\n",
       " 913: 'agreed',\n",
       " 914: 'audio',\n",
       " 915: '72',\n",
       " 916: 'estimate',\n",
       " 917: 'original',\n",
       " 918: 'amount',\n",
       " 919: '01',\n",
       " 920: 'tom',\n",
       " 921: \"weren't\",\n",
       " 922: 'scotland',\n",
       " 923: 'fucked',\n",
       " 924: 'welcome',\n",
       " 925: 'beer',\n",
       " 926: 'drinking',\n",
       " 927: 'green',\n",
       " 928: 'selling',\n",
       " 929: 'uh',\n",
       " 930: 'payer',\n",
       " 931: \"state'\",\n",
       " 932: 'upvote',\n",
       " 933: 'voting',\n",
       " 934: 'view',\n",
       " 935: 'hate',\n",
       " 936: 'sacrifice',\n",
       " 937: 'sake',\n",
       " 938: 'games',\n",
       " 939: 'harass',\n",
       " 940: 'sucks',\n",
       " 941: 'changes',\n",
       " 942: 'evil',\n",
       " 943: 'diversion',\n",
       " 944: 'february',\n",
       " 945: 'san',\n",
       " 946: 'saddam',\n",
       " 947: 'found',\n",
       " 948: 'claims',\n",
       " 949: 'report',\n",
       " 950: 'hour',\n",
       " 951: 'mistakes',\n",
       " 952: 'came',\n",
       " 953: 'definition',\n",
       " 954: 'memory',\n",
       " 955: 'losers',\n",
       " 956: 'normal',\n",
       " 957: 'liberals',\n",
       " 958: 'hilarious',\n",
       " 959: 'model',\n",
       " 960: 'dvd',\n",
       " 961: 'fluid',\n",
       " 962: 'ounces',\n",
       " 963: 'index',\n",
       " 964: 'rich',\n",
       " 965: \"kucinich's\",\n",
       " 966: 'republicans',\n",
       " 967: 'raise',\n",
       " 968: 'pounds',\n",
       " 969: 'virtual',\n",
       " 970: 'shark',\n",
       " 971: 'boat',\n",
       " 972: 'salsa',\n",
       " 973: 'agreement',\n",
       " 974: 'ways',\n",
       " 975: 'elsewhere',\n",
       " 976: 'decision',\n",
       " 977: 'paying',\n",
       " 978: 'decisions',\n",
       " 979: 'design',\n",
       " 980: 'decide',\n",
       " 981: 'killer',\n",
       " 982: 'street',\n",
       " 983: 'name',\n",
       " 984: 'crazy',\n",
       " 985: 'bastard',\n",
       " 986: 'prize',\n",
       " 987: \"they'll\",\n",
       " 988: 'future',\n",
       " 989: 'dunno',\n",
       " 990: 'closer',\n",
       " 991: 'quarter',\n",
       " 992: 'scared',\n",
       " 993: 'break',\n",
       " 994: 'apply',\n",
       " 995: 'admire',\n",
       " 996: 'fan',\n",
       " 997: 'lives',\n",
       " 998: 'anybody',\n",
       " 999: 'zoom',\n",
       " 1000: 'program',\n",
       " ...}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reverse_target_char_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate target given source sequence\n",
    "def predict_sequence(infenc, infdec, source, n_steps, cardinality):\n",
    "\t# encode\n",
    "\tstate = infenc.predict(source)\n",
    "\t# start of sequence input\n",
    "\ttarget_seq = array([0.0 for _ in range(cardinality)]).reshape(1, 1, cardinality)\n",
    "\t# collect predictions\n",
    "\toutput = list()\n",
    "\tfor t in range(n_steps):\n",
    "\t\t# predict next char\n",
    "\t\tyhat, h, c = infdec.predict([target_seq] + state)\n",
    "\t\t# store prediction\n",
    "\t\toutput.append(yhat[0,0,:])\n",
    "\t\t# update state\n",
    "\t\tstate = [h, c]\n",
    "\t\t# update target sequence\n",
    "\t\ttarget_seq = yhat\n",
    "\treturn array(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'\\t'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-a1fba762bdbd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[1;31m# for trying out decoding.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[0minput_seq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainInputEncoded\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mseq_index\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mseq_index\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m     \u001b[0mdecoded_sentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecode_sequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_seq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'-'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Input sentence:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_texts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mseq_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-38-a1fba762bdbd>\u001b[0m in \u001b[0;36mdecode_sequence\u001b[1;34m(input_seq)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mtarget_seq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnDecoderToken\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;31m# Populate the first character of target sequence with the start character.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mtarget_seq\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTargetindex\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'\\t'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;31m# Sampling loop for a batch of sequences\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: '\\t'"
     ]
    }
   ],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, nDecoderToken))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, Targetindex['\\t']] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length or find stop character.\n",
    "        if (sampled_char == '\\n' or len(decoded_sentence) > max_len):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, nDecoderToken))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence\n",
    "\n",
    "\n",
    "for seq_index in range(100):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "    input_seq = trainInputEncoded[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
