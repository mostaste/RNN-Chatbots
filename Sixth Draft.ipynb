{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.2.4'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "from keras import preprocessing\n",
    "from keras import optimizers\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers import SeparableConv1D, MaxPooling1D\n",
    "from keras.layers import Input\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import GRU\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Flatten\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "does this also apply to the salad?\n",
      "jesus, i hope not.\n",
      "Found 29620 sentences from TrainFrom Text\n",
      "Found 29620 sentences from TrainTo Text\n",
      "Found 521666 words from TrainFrom Text\n",
      "Found 479824 words from TrainTo Text\n"
     ]
    }
   ],
   "source": [
    "trainFromTextFile = \"train.FROM\"\n",
    "trainToTextFile   = \"train.TO\"\n",
    "trainFromText     = open(trainFromTextFile, 'r', encoding='utf-8').read().lower()\n",
    "trainToText       = open(trainToTextFile, 'r', encoding='utf-8').read().lower()\n",
    "trainFromSentenceTokens = re.split('\\n', trainFromText)\n",
    "trainToSentenceTokens   = re.split('\\n', trainToText)\n",
    "trainFromWordTokens = re.split(' |\\n', trainFromText)\n",
    "trainToWordTokens   = re.split(' |\\n', trainToText)\n",
    "\n",
    "print('Found %s sentences from TrainFrom Text' %len(trainFromSentenceTokens))\n",
    "print('Found %s sentences from TrainTo Text' %len(trainToSentenceTokens))\n",
    "print('Found %s words from TrainFrom Text' %len(trainFromWordTokens))\n",
    "print('Found %s words from TrainTo Text' %len(trainToWordTokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "does\n",
      "jesus,\n",
      "does this also apply to the salad?\n",
      "jesus, i hope not.\n"
     ]
    }
   ],
   "source": [
    "print(trainFromWordTokens[0])\n",
    "print(trainToWordTokens[0])\n",
    "print(trainFromSentenceTokens[0])\n",
    "print(trainToSentenceTokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainInputTokens = trainFromSentenceTokens[0:1000]\n",
    "trainTargetTokens = trainToSentenceTokens[0:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train From File:\n",
      "\n",
      "Found 1000 sentences.\n",
      "Found 1000 sequences.\n",
      "Found 4304 unique tokens.\n",
      "Found 4304 unique words.\n"
     ]
    }
   ],
   "source": [
    "max_len = 100    # We will cut comments after 100 words\n",
    "max_words = 10000  # We will only consider the top 10,000 words in the dataset\n",
    "\n",
    "tokenizerInput = Tokenizer(num_words=max_words)\n",
    "tokenizerInput.fit_on_texts(trainInputTokens)\n",
    "\n",
    "sequencesInput = tokenizerInput.texts_to_sequences(trainInputTokens)\n",
    "sequencesInput = pad_sequences(sequencesInput, maxlen=max_len)  #Pad so all the arrays are the same size\n",
    "\n",
    "Inputindex = tokenizerInput.word_index\n",
    "Inputcount = tokenizerInput.word_counts\n",
    "nInput = len(tokenizerInput.word_counts) + 1\n",
    "\n",
    "print(\"Train From File:\\n\")\n",
    "print('Found %s sentences.' %len(trainInputTokens))\n",
    "print('Found %s sequences.' %len(sequencesInput))\n",
    "print('Found %s unique tokens.' % len(Inputindex))\n",
    "print('Found %s unique words.' % len(Inputcount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,  102,   22,  108, 1505,    3,    1,\n",
       "       1506])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequencesInput[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'seems like the wildest and craziest guys are pastors of megachurches.  those guys know how to have fun.  republican senators and representatives are pretty wild too.'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainInputTokens[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 100)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequencesInput.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train To File:\n",
      "\n",
      "Found 1000 sentences.\n",
      "Found 1000 sequences.\n",
      "Found 4129 unique tokens.\n",
      "Found 4129 unique words.\n"
     ]
    }
   ],
   "source": [
    "tokenizerTarget = Tokenizer(num_words=max_words)\n",
    "tokenizerTarget.fit_on_texts(trainTargetTokens)\n",
    "\n",
    "Targetindex = tokenizerTarget.word_index\n",
    "Targetcount = tokenizerTarget.word_counts\n",
    "nTarget     = len(tokenizerTarget.word_counts) + 1\n",
    "\n",
    "sequencesTarget = tokenizerTarget.texts_to_sequences(trainTargetTokens)\n",
    "sequencesTarget = pad_sequences(sequencesTarget, maxlen=max_len)  #Pad so all the arrays are the same size\n",
    "\n",
    "print(\"Train To File:\\n\")\n",
    "print('Found %s sentences.' %len(trainTargetTokens))\n",
    "print('Found %s sequences.' %len(sequencesTarget))\n",
    "print('Found %s unique tokens.' % len(Targetindex))\n",
    "print('Found %s unique words.' % len(Targetcount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jesus, i hope not.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainTargetTokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 100)\n",
      "(1000, 100)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'the': 1,\n",
       " 'newlinechar': 2,\n",
       " 'to': 3,\n",
       " 'a': 4,\n",
       " 'i': 5,\n",
       " 'you': 6,\n",
       " 'and': 7,\n",
       " 'of': 8,\n",
       " 'is': 9,\n",
       " 'that': 10,\n",
       " 'it': 11,\n",
       " 'in': 12,\n",
       " 'for': 13,\n",
       " 'on': 14,\n",
       " 'not': 15,\n",
       " 'they': 16,\n",
       " 'be': 17,\n",
       " 'have': 18,\n",
       " 'was': 19,\n",
       " 'are': 20,\n",
       " 'but': 21,\n",
       " 'if': 22,\n",
       " 'with': 23,\n",
       " \"it's\": 24,\n",
       " 'just': 25,\n",
       " 'my': 26,\n",
       " 'your': 27,\n",
       " 'or': 28,\n",
       " 'like': 29,\n",
       " 'as': 30,\n",
       " 'would': 31,\n",
       " 'all': 32,\n",
       " 'what': 33,\n",
       " 'this': 34,\n",
       " \"don't\": 35,\n",
       " 'so': 36,\n",
       " 'he': 37,\n",
       " 'http': 38,\n",
       " 'think': 39,\n",
       " 'people': 40,\n",
       " 'do': 41,\n",
       " 'me': 42,\n",
       " 'no': 43,\n",
       " 'at': 44,\n",
       " \"'\": 45,\n",
       " 'from': 46,\n",
       " 'one': 47,\n",
       " 'can': 48,\n",
       " 'gt': 49,\n",
       " 'how': 50,\n",
       " 'there': 51,\n",
       " 'about': 52,\n",
       " 'more': 53,\n",
       " 'we': 54,\n",
       " 'get': 55,\n",
       " 'know': 56,\n",
       " 'com': 57,\n",
       " 'some': 58,\n",
       " 'them': 59,\n",
       " 'then': 60,\n",
       " 'an': 61,\n",
       " \"that's\": 62,\n",
       " 'will': 63,\n",
       " 'up': 64,\n",
       " \"i'm\": 65,\n",
       " 'good': 66,\n",
       " 'well': 67,\n",
       " 'by': 68,\n",
       " 'who': 69,\n",
       " 'because': 70,\n",
       " 'make': 71,\n",
       " 'than': 72,\n",
       " 'their': 73,\n",
       " 'his': 74,\n",
       " 'too': 75,\n",
       " 'when': 76,\n",
       " 'www': 77,\n",
       " 'out': 78,\n",
       " 'why': 79,\n",
       " 'actually': 80,\n",
       " 'which': 81,\n",
       " 'reddit': 82,\n",
       " 'has': 83,\n",
       " 'should': 84,\n",
       " 'point': 85,\n",
       " 'only': 86,\n",
       " 'back': 87,\n",
       " 'time': 88,\n",
       " 'did': 89,\n",
       " 'see': 90,\n",
       " 'were': 91,\n",
       " 'go': 92,\n",
       " 'even': 93,\n",
       " 'still': 94,\n",
       " 'most': 95,\n",
       " 'now': 96,\n",
       " 'man': 97,\n",
       " \"you're\": 98,\n",
       " 'much': 99,\n",
       " 'could': 100,\n",
       " 'those': 101,\n",
       " \"didn't\": 102,\n",
       " 'other': 103,\n",
       " 'really': 104,\n",
       " 'mean': 105,\n",
       " 'used': 106,\n",
       " \"can't\": 107,\n",
       " \"doesn't\": 108,\n",
       " 'maybe': 109,\n",
       " 'any': 110,\n",
       " 'us': 111,\n",
       " 'first': 112,\n",
       " 'yes': 113,\n",
       " 'thing': 114,\n",
       " 'want': 115,\n",
       " 'been': 116,\n",
       " 'here': 117,\n",
       " 'does': 118,\n",
       " \"he's\": 119,\n",
       " 'yeah': 120,\n",
       " 'amp': 121,\n",
       " 'being': 122,\n",
       " 'right': 123,\n",
       " 'off': 124,\n",
       " 'comment': 125,\n",
       " 'someone': 126,\n",
       " 'got': 127,\n",
       " 'sure': 128,\n",
       " 'again': 129,\n",
       " 'read': 130,\n",
       " '2': 131,\n",
       " 'better': 132,\n",
       " 'down': 133,\n",
       " 'over': 134,\n",
       " 'many': 135,\n",
       " 'into': 136,\n",
       " 'true': 137,\n",
       " 'very': 138,\n",
       " 'day': 139,\n",
       " 'saying': 140,\n",
       " 'said': 141,\n",
       " 'watch': 142,\n",
       " 'thought': 143,\n",
       " 'guy': 144,\n",
       " 'had': 145,\n",
       " 'way': 146,\n",
       " 'everyone': 147,\n",
       " 'probably': 148,\n",
       " 'pretty': 149,\n",
       " 'our': 150,\n",
       " 'god': 151,\n",
       " 'sorry': 152,\n",
       " 'put': 153,\n",
       " 'where': 154,\n",
       " '1': 155,\n",
       " 'having': 156,\n",
       " 'youtube': 157,\n",
       " 'say': 158,\n",
       " 'thanks': 159,\n",
       " \"isn't\": 160,\n",
       " 'paul': 161,\n",
       " 'believe': 162,\n",
       " 'money': 163,\n",
       " 'call': 164,\n",
       " 'already': 165,\n",
       " 'oh': 166,\n",
       " 'least': 167,\n",
       " 'agree': 168,\n",
       " 'use': 169,\n",
       " 'without': 170,\n",
       " 'long': 171,\n",
       " 'same': 172,\n",
       " 'org': 173,\n",
       " 'two': 174,\n",
       " 'its': 175,\n",
       " 'free': 176,\n",
       " 'ever': 177,\n",
       " 'also': 178,\n",
       " 'tell': 179,\n",
       " 'life': 180,\n",
       " \"i'd\": 181,\n",
       " 'anyone': 182,\n",
       " 'take': 183,\n",
       " 'great': 184,\n",
       " 'might': 185,\n",
       " 'find': 186,\n",
       " 'always': 187,\n",
       " 'bad': 188,\n",
       " 'enough': 189,\n",
       " 'nothing': 190,\n",
       " 'though': 191,\n",
       " 'something': 192,\n",
       " 'these': 193,\n",
       " 'world': 194,\n",
       " 'give': 195,\n",
       " 'around': 196,\n",
       " 'system': 197,\n",
       " 'never': 198,\n",
       " \"they're\": 199,\n",
       " 'every': 200,\n",
       " 'look': 201,\n",
       " \"wasn't\": 202,\n",
       " 'little': 203,\n",
       " 'against': 204,\n",
       " 'things': 205,\n",
       " 'after': 206,\n",
       " 'guess': 207,\n",
       " 'o': 208,\n",
       " \"i've\": 209,\n",
       " 'government': 210,\n",
       " 'wait': 211,\n",
       " 'v': 212,\n",
       " 'while': 213,\n",
       " 'may': 214,\n",
       " 'code': 215,\n",
       " 'quite': 216,\n",
       " 'case': 217,\n",
       " 'exactly': 218,\n",
       " 'own': 219,\n",
       " 'made': 220,\n",
       " 'yet': 221,\n",
       " 'using': 222,\n",
       " 'reason': 223,\n",
       " 'him': 224,\n",
       " 'real': 225,\n",
       " 'job': 226,\n",
       " 'need': 227,\n",
       " 'seems': 228,\n",
       " 'such': 229,\n",
       " 'html': 230,\n",
       " 'wikipedia': 231,\n",
       " 'come': 232,\n",
       " 'work': 233,\n",
       " 'going': 234,\n",
       " 'norway': 235,\n",
       " 'wrong': 236,\n",
       " 'x': 237,\n",
       " \"there's\": 238,\n",
       " 'please': 239,\n",
       " 'makes': 240,\n",
       " 'doing': 241,\n",
       " 'both': 242,\n",
       " 'before': 243,\n",
       " 'buy': 244,\n",
       " 'write': 245,\n",
       " \"what's\": 246,\n",
       " 'kind': 247,\n",
       " 'part': 248,\n",
       " 'yourself': 249,\n",
       " 'stuff': 250,\n",
       " 'seen': 251,\n",
       " 'new': 252,\n",
       " 'bush': 253,\n",
       " 'starting': 254,\n",
       " 'years': 255,\n",
       " 'different': 256,\n",
       " 'once': 257,\n",
       " 'source': 258,\n",
       " 'fucking': 259,\n",
       " 'comments': 260,\n",
       " 'lot': 261,\n",
       " 'talking': 262,\n",
       " 'else': 263,\n",
       " 'under': 264,\n",
       " 'mysql': 265,\n",
       " 'likely': 266,\n",
       " 'oil': 267,\n",
       " 'article': 268,\n",
       " 'votes': 269,\n",
       " 'far': 270,\n",
       " 'books': 271,\n",
       " 'hope': 272,\n",
       " 'pay': 273,\n",
       " 'until': 274,\n",
       " 'order': 275,\n",
       " 'away': 276,\n",
       " 'she': 277,\n",
       " 'less': 278,\n",
       " 'through': 279,\n",
       " 'head': 280,\n",
       " 'trying': 281,\n",
       " 'freedom': 282,\n",
       " \"i'll\": 283,\n",
       " 'am': 284,\n",
       " 'video': 285,\n",
       " 'hard': 286,\n",
       " 's': 287,\n",
       " 'means': 288,\n",
       " 'story': 289,\n",
       " 'last': 290,\n",
       " 'ah': 291,\n",
       " \"we're\": 292,\n",
       " 'meant': 293,\n",
       " 'getting': 294,\n",
       " 'question': 295,\n",
       " 'argument': 296,\n",
       " 'old': 297,\n",
       " 'choice': 298,\n",
       " \"wouldn't\": 299,\n",
       " 'her': 300,\n",
       " 'fact': 301,\n",
       " 'fuck': 302,\n",
       " 'en': 303,\n",
       " 'wiki': 304,\n",
       " 'perhaps': 305,\n",
       " 'help': 306,\n",
       " 'clear': 307,\n",
       " 'keep': 308,\n",
       " 'joke': 309,\n",
       " 'understand': 310,\n",
       " 'kill': 311,\n",
       " 'car': 312,\n",
       " 'linux': 313,\n",
       " 'funny': 314,\n",
       " 'called': 315,\n",
       " 'instead': 316,\n",
       " 'support': 317,\n",
       " 'imagine': 318,\n",
       " \"you've\": 319,\n",
       " 'hell': 320,\n",
       " 'indeed': 321,\n",
       " 'book': 322,\n",
       " 'show': 323,\n",
       " 'wish': 324,\n",
       " 'left': 325,\n",
       " 'everything': 326,\n",
       " 'remember': 327,\n",
       " 'stop': 328,\n",
       " 'awesome': 329,\n",
       " 'lol': 330,\n",
       " 'lt': 331,\n",
       " 'literally': 332,\n",
       " 'days': 333,\n",
       " 'serious': 334,\n",
       " 'rather': 335,\n",
       " 'goes': 336,\n",
       " 'house': 337,\n",
       " 'making': 338,\n",
       " 'bit': 339,\n",
       " 'google': 340,\n",
       " '3d0': 341,\n",
       " 'change': 342,\n",
       " 'feel': 343,\n",
       " 'content': 344,\n",
       " 'p': 345,\n",
       " 'sense': 346,\n",
       " 'next': 347,\n",
       " 'gets': 348,\n",
       " 'roll': 349,\n",
       " \"'the\": 350,\n",
       " 'damn': 351,\n",
       " 'link': 352,\n",
       " 'thank': 353,\n",
       " 'net': 354,\n",
       " 'language': 355,\n",
       " 'thinking': 356,\n",
       " 'fun': 357,\n",
       " 'points': 358,\n",
       " 'thats': 359,\n",
       " 'consider': 360,\n",
       " 'c': 361,\n",
       " 'info': 362,\n",
       " 'comes': 363,\n",
       " 'ron': 364,\n",
       " 'place': 365,\n",
       " 'whole': 366,\n",
       " 'able': 367,\n",
       " 'big': 368,\n",
       " 'small': 369,\n",
       " 'pizza': 370,\n",
       " 'shit': 371,\n",
       " 'cool': 372,\n",
       " 'must': 373,\n",
       " 'game': 374,\n",
       " 'vi': 375,\n",
       " 'looks': 376,\n",
       " 'uk': 377,\n",
       " 'non': 378,\n",
       " 'start': 379,\n",
       " 'history': 380,\n",
       " 'knows': 381,\n",
       " 'second': 382,\n",
       " 'click': 383,\n",
       " 'says': 384,\n",
       " 'mention': 385,\n",
       " 'obviously': 386,\n",
       " 'top': 387,\n",
       " 'each': 388,\n",
       " 'anyway': 389,\n",
       " 'sir': 390,\n",
       " 'yep': 391,\n",
       " 'experience': 392,\n",
       " 'end': 393,\n",
       " 'mccain': 394,\n",
       " 'national': 395,\n",
       " 'news': 396,\n",
       " \"haven't\": 397,\n",
       " 'page': 398,\n",
       " 'version': 399,\n",
       " 'rule': 400,\n",
       " 'running': 401,\n",
       " 'friends': 402,\n",
       " 'hit': 403,\n",
       " 'myself': 404,\n",
       " 'america': 405,\n",
       " 'whether': 406,\n",
       " \"he'll\": 407,\n",
       " '3': 408,\n",
       " 'machine': 409,\n",
       " 'seem': 410,\n",
       " 'wanted': 411,\n",
       " 'country': 412,\n",
       " 'sheeple': 413,\n",
       " 'gonna': 414,\n",
       " 'except': 415,\n",
       " 'voted': 416,\n",
       " 'open': 417,\n",
       " 'heard': 418,\n",
       " 'basically': 419,\n",
       " 'parents': 420,\n",
       " 'run': 421,\n",
       " 'best': 422,\n",
       " \"won't\": 423,\n",
       " \"aren't\": 424,\n",
       " 'sounds': 425,\n",
       " 'word': 426,\n",
       " 'election': 427,\n",
       " 'gay': 428,\n",
       " 'behind': 429,\n",
       " 'jpg': 430,\n",
       " \"here's\": 431,\n",
       " 'post': 432,\n",
       " 'took': 433,\n",
       " '4': 434,\n",
       " 'etc': 435,\n",
       " '20': 436,\n",
       " 'definitely': 437,\n",
       " 'either': 438,\n",
       " 'face': 439,\n",
       " 'person': 440,\n",
       " \"couldn't\": 441,\n",
       " 'edit': 442,\n",
       " \"you'd\": 443,\n",
       " 'air': 444,\n",
       " \"let's\": 445,\n",
       " 'vote': 446,\n",
       " 'count': 447,\n",
       " 'editor': 448,\n",
       " 'public': 449,\n",
       " 'year': 450,\n",
       " 'anymore': 451,\n",
       " 'button': 452,\n",
       " 'minutes': 453,\n",
       " 'course': 454,\n",
       " 'fault': 455,\n",
       " 'nobody': 456,\n",
       " 'intelligent': 457,\n",
       " 'gives': 458,\n",
       " 'holy': 459,\n",
       " 'police': 460,\n",
       " 'child': 461,\n",
       " 'g': 462,\n",
       " 'media': 463,\n",
       " 'internet': 464,\n",
       " 'played': 465,\n",
       " 'comic': 466,\n",
       " 'between': 467,\n",
       " 'wonder': 468,\n",
       " '11': 469,\n",
       " 'hunting': 470,\n",
       " 'love': 471,\n",
       " 'die': 472,\n",
       " 'web': 473,\n",
       " 'board': 474,\n",
       " 'fine': 475,\n",
       " 'music': 476,\n",
       " 'price': 477,\n",
       " 'trouble': 478,\n",
       " 'mine': 479,\n",
       " 'apple': 480,\n",
       " 'water': 481,\n",
       " 'democrats': 482,\n",
       " 'cut': 483,\n",
       " 'cage': 484,\n",
       " 'high': 485,\n",
       " 'property': 486,\n",
       " 'killed': 487,\n",
       " 'past': 488,\n",
       " 'almost': 489,\n",
       " 'working': 490,\n",
       " 'canada': 491,\n",
       " 'law': 492,\n",
       " 'tree': 493,\n",
       " 'brilliant': 494,\n",
       " 'iraq': 495,\n",
       " '2byears': 496,\n",
       " 'picture': 497,\n",
       " 'try': 498,\n",
       " 'couple': 499,\n",
       " 'lack': 500,\n",
       " 'claim': 501,\n",
       " 'scientology': 502,\n",
       " 'hardware': 503,\n",
       " 'side': 504,\n",
       " 'title': 505,\n",
       " 'culture': 506,\n",
       " 'bill': 507,\n",
       " 'whatever': 508,\n",
       " 'data': 509,\n",
       " 'care': 510,\n",
       " 'reference': 511,\n",
       " 'forget': 512,\n",
       " 'told': 513,\n",
       " 'south': 514,\n",
       " 'ad': 515,\n",
       " 'dangerous': 516,\n",
       " 'tried': 517,\n",
       " 'mind': 518,\n",
       " 'close': 519,\n",
       " 'red': 520,\n",
       " 'environment': 521,\n",
       " 'closed': 522,\n",
       " 'handle': 523,\n",
       " 'mail': 524,\n",
       " 'thinks': 525,\n",
       " 'interesting': 526,\n",
       " 'information': 527,\n",
       " 'truck': 528,\n",
       " 'went': 529,\n",
       " 'hold': 530,\n",
       " 'dollars': 531,\n",
       " 'stories': 532,\n",
       " 'business': 533,\n",
       " 'needs': 534,\n",
       " 'continue': 535,\n",
       " 'certainly': 536,\n",
       " 'deal': 537,\n",
       " 'done': 538,\n",
       " 'ok': 539,\n",
       " 'few': 540,\n",
       " '5': 541,\n",
       " 'neither': 542,\n",
       " 'move': 543,\n",
       " 'phrase': 544,\n",
       " 'girl': 545,\n",
       " 'considering': 546,\n",
       " 'assume': 547,\n",
       " 'nah': 548,\n",
       " \"you'll\": 549,\n",
       " 'laugh': 550,\n",
       " 'personally': 551,\n",
       " 'happen': 552,\n",
       " 'religion': 553,\n",
       " 'seriously': 554,\n",
       " 'eat': 555,\n",
       " 'since': 556,\n",
       " 'learn': 557,\n",
       " 'five': 558,\n",
       " 'nearly': 559,\n",
       " 'online': 560,\n",
       " 'wow': 561,\n",
       " 'soon': 562,\n",
       " 'previous': 563,\n",
       " 'million': 564,\n",
       " 'practically': 565,\n",
       " 'ftw': 566,\n",
       " 'machines': 567,\n",
       " 'error': 568,\n",
       " 'fair': 569,\n",
       " 'quality': 570,\n",
       " 'hackers': 571,\n",
       " 'app': 572,\n",
       " 'press': 573,\n",
       " 'walmart': 574,\n",
       " 'card': 575,\n",
       " 'jesus': 576,\n",
       " 'foreign': 577,\n",
       " '000': 578,\n",
       " 'plus': 579,\n",
       " 'members': 580,\n",
       " 'others': 581,\n",
       " 'church': 582,\n",
       " 'full': 583,\n",
       " 'honestly': 584,\n",
       " 'compared': 585,\n",
       " 'available': 586,\n",
       " 'republican': 587,\n",
       " 'note': 588,\n",
       " 'bed': 589,\n",
       " 'dead': 590,\n",
       " 'implemented': 591,\n",
       " 'win': 592,\n",
       " 'sentence': 593,\n",
       " 'anything': 594,\n",
       " 'takes': 595,\n",
       " 'rational': 596,\n",
       " 'research': 597,\n",
       " 'thread': 598,\n",
       " 'php': 599,\n",
       " 'hat': 600,\n",
       " 'likes': 601,\n",
       " 'nuts': 602,\n",
       " 'save': 603,\n",
       " 'commute': 604,\n",
       " 'california': 605,\n",
       " 'ass': 606,\n",
       " 'obama': 607,\n",
       " 'race': 608,\n",
       " 'cbs': 609,\n",
       " 'unless': 610,\n",
       " 'huge': 611,\n",
       " 'illegal': 612,\n",
       " 'taking': 613,\n",
       " 'b': 614,\n",
       " 'sign': 615,\n",
       " 'looking': 616,\n",
       " 'line': 617,\n",
       " 'suppose': 618,\n",
       " 'divide': 619,\n",
       " 'growth': 620,\n",
       " 'double': 621,\n",
       " 'notice': 622,\n",
       " 'cruise': 623,\n",
       " 'president': 624,\n",
       " 'de': 625,\n",
       " 'live': 626,\n",
       " 'corporate': 627,\n",
       " 'tax': 628,\n",
       " 'dude': 629,\n",
       " 'necessarily': 630,\n",
       " 'sick': 631,\n",
       " 'biggest': 632,\n",
       " 'add': 633,\n",
       " 'security': 634,\n",
       " 'privacy': 635,\n",
       " 'ronnie': 636,\n",
       " 'fix': 637,\n",
       " 'situation': 638,\n",
       " 'effect': 639,\n",
       " 'bong': 640,\n",
       " 'whose': 641,\n",
       " 'administration': 642,\n",
       " 'american': 643,\n",
       " 'easier': 644,\n",
       " 'potential': 645,\n",
       " 'common': 646,\n",
       " 'created': 647,\n",
       " 'image': 648,\n",
       " 'emphasize': 649,\n",
       " '9': 650,\n",
       " 'giving': 651,\n",
       " 'difference': 652,\n",
       " 'bought': 653,\n",
       " \"12'\": 654,\n",
       " 'market': 655,\n",
       " 'built': 656,\n",
       " 'gallon': 657,\n",
       " 'insane': 658,\n",
       " 'nice': 659,\n",
       " '10': 660,\n",
       " 'cheaper': 661,\n",
       " 'inside': 662,\n",
       " 'bigger': 663,\n",
       " 'wanna': 664,\n",
       " 'sell': 665,\n",
       " 'sleep': 666,\n",
       " 'friend': 667,\n",
       " 'products': 668,\n",
       " 'pass': 669,\n",
       " 'laptop': 670,\n",
       " 'production': 671,\n",
       " 'fell': 672,\n",
       " 'poor': 673,\n",
       " 'entire': 674,\n",
       " 'irrational': 675,\n",
       " 'private': 676,\n",
       " 'none': 677,\n",
       " 'upmodded': 678,\n",
       " 'docid': 679,\n",
       " 'q': 680,\n",
       " 'possibly': 681,\n",
       " 'software': 682,\n",
       " 'three': 683,\n",
       " 'programming': 684,\n",
       " 'libertarian': 685,\n",
       " 'disagree': 686,\n",
       " 'explain': 687,\n",
       " 'replace': 688,\n",
       " 'torture': 689,\n",
       " 'understanding': 690,\n",
       " 'racist': 691,\n",
       " 'happens': 692,\n",
       " 'apples': 693,\n",
       " 'mr': 694,\n",
       " 'however': 695,\n",
       " 'another': 696,\n",
       " 'everybody': 697,\n",
       " 're': 698,\n",
       " 'lucky': 699,\n",
       " 'prove': 700,\n",
       " 'check': 701,\n",
       " 'hand': 702,\n",
       " 'alive': 703,\n",
       " 'county': 704,\n",
       " 'states': 705,\n",
       " 'candidate': 706,\n",
       " 'wind': 707,\n",
       " 'missed': 708,\n",
       " 'actual': 709,\n",
       " 'specific': 710,\n",
       " 'ruby': 711,\n",
       " 'home': 712,\n",
       " 'htm': 713,\n",
       " 'involve': 714,\n",
       " 'risk': 715,\n",
       " '2007': 716,\n",
       " 'happy': 717,\n",
       " 'numbers': 718,\n",
       " 'safety': 719,\n",
       " \"'it's\": 720,\n",
       " 'suck': 721,\n",
       " 'power': 722,\n",
       " 'reading': 723,\n",
       " 'tools': 724,\n",
       " 'e': 725,\n",
       " 'example': 726,\n",
       " 'black': 727,\n",
       " 'h': 728,\n",
       " 'hot': 729,\n",
       " 'fail': 730,\n",
       " 'play': 731,\n",
       " \"they've\": 732,\n",
       " 'known': 733,\n",
       " 'site': 734,\n",
       " 'started': 735,\n",
       " 'giant': 736,\n",
       " 'followers': 737,\n",
       " 'response': 738,\n",
       " 'works': 739,\n",
       " 'postgres': 740,\n",
       " 'standards': 741,\n",
       " 'set': 742,\n",
       " 'gotta': 743,\n",
       " 'false': 744,\n",
       " 'worse': 745,\n",
       " 'plastic': 746,\n",
       " 'totally': 747,\n",
       " 'dollar': 748,\n",
       " 'completely': 749,\n",
       " 'problem': 750,\n",
       " 'depending': 751,\n",
       " 'sun': 752,\n",
       " 'force': 753,\n",
       " 'doctor': 754,\n",
       " 'limited': 755,\n",
       " 'dhurka': 756,\n",
       " 'reasons': 757,\n",
       " 'state': 758,\n",
       " 'saw': 759,\n",
       " 'deliver': 760,\n",
       " 'talk': 761,\n",
       " 'highly': 762,\n",
       " 'im': 763,\n",
       " 'kid': 764,\n",
       " 'club': 765,\n",
       " 'harper': 766,\n",
       " 'opinion': 767,\n",
       " 'facts': 768,\n",
       " 'based': 769,\n",
       " 'sweden': 770,\n",
       " 'babies': 771,\n",
       " 'allow': 772,\n",
       " 'boy': 773,\n",
       " 'postgresql': 774,\n",
       " 'corner': 775,\n",
       " 'company': 776,\n",
       " 'sent': 777,\n",
       " 'straight': 778,\n",
       " 'service': 779,\n",
       " 'asking': 780,\n",
       " 'expect': 781,\n",
       " 'voice': 782,\n",
       " 'traffic': 783,\n",
       " '90': 784,\n",
       " 'quick': 785,\n",
       " 'politics': 786,\n",
       " 'correct': 787,\n",
       " 'age': 788,\n",
       " 'anti': 789,\n",
       " 'ask': 790,\n",
       " 'rejected': 791,\n",
       " 'insert': 792,\n",
       " '100': 793,\n",
       " 'context': 794,\n",
       " 'deep': 795,\n",
       " 'window': 796,\n",
       " 'sync': 797,\n",
       " 'unfortunately': 798,\n",
       " 'result': 799,\n",
       " 'fertility': 800,\n",
       " 'possible': 801,\n",
       " 'perl': 802,\n",
       " 'prefer': 803,\n",
       " 'choices': 804,\n",
       " 'duck': 805,\n",
       " 'hey': 806,\n",
       " 'pictures': 807,\n",
       " 'sort': 808,\n",
       " 'crap': 809,\n",
       " 'lie': 810,\n",
       " 'docs': 811,\n",
       " 'seeing': 812,\n",
       " 'rest': 813,\n",
       " \"'i\": 814,\n",
       " 'rain': 815,\n",
       " 'looked': 816,\n",
       " 'male': 817,\n",
       " 'zero': 818,\n",
       " 'uses': 819,\n",
       " 'learned': 820,\n",
       " 'school': 821,\n",
       " 'choose': 822,\n",
       " 'women': 823,\n",
       " 'choosing': 824,\n",
       " 'children': 825,\n",
       " 'type': 826,\n",
       " 'stake': 827,\n",
       " 'glider': 828,\n",
       " 'personal': 829,\n",
       " 'sweet': 830,\n",
       " 'genocide': 831,\n",
       " 'idea': 832,\n",
       " 'ideas': 833,\n",
       " 'vim': 834,\n",
       " 'stupid': 835,\n",
       " 'debit': 836,\n",
       " 'windows': 837,\n",
       " 'ed': 838,\n",
       " 'restroom': 839,\n",
       " 'sex': 840,\n",
       " 'nationals': 841,\n",
       " 'universities': 842,\n",
       " 'charge': 843,\n",
       " 'students': 844,\n",
       " 'taxation': 845,\n",
       " 'eu': 846,\n",
       " 'christian': 847,\n",
       " 'later': 848,\n",
       " 'worth': 849,\n",
       " 'macbook': 850,\n",
       " 'pro': 851,\n",
       " 'user': 852,\n",
       " '99': 853,\n",
       " 'carefully': 854,\n",
       " 'figures': 855,\n",
       " 'heavily': 856,\n",
       " 'hex': 857,\n",
       " 'shirt': 858,\n",
       " 'coder': 859,\n",
       " \"we'd\": 860,\n",
       " 'bashing': 861,\n",
       " 'typing': 862,\n",
       " 'action': 863,\n",
       " 'record': 864,\n",
       " 'logic': 865,\n",
       " 'due': 866,\n",
       " \"microsoft's\": 867,\n",
       " 'posted': 868,\n",
       " 'redditors': 869,\n",
       " 'twins': 870,\n",
       " 'java': 871,\n",
       " 'military': 872,\n",
       " 'argue': 873,\n",
       " 'xenu': 874,\n",
       " 'credit': 875,\n",
       " 'steel': 876,\n",
       " \"things'\": 877,\n",
       " 'near': 878,\n",
       " 'bottom': 879,\n",
       " 'systems': 880,\n",
       " \"america's\": 881,\n",
       " 'fits': 882,\n",
       " 'texas': 883,\n",
       " 'd': 884,\n",
       " 'wins': 885,\n",
       " 'protest': 886,\n",
       " 'golden': 887,\n",
       " 'local': 888,\n",
       " 'tv': 889,\n",
       " 'normally': 890,\n",
       " 'broadcast': 891,\n",
       " 'n': 892,\n",
       " 'mentioned': 893,\n",
       " 'porn': 894,\n",
       " \"'media'\": 895,\n",
       " 'movies': 896,\n",
       " 'naked': 897,\n",
       " 'covers': 898,\n",
       " 'anywhere': 899,\n",
       " 'besides': 900,\n",
       " 'judgements': 901,\n",
       " 'character': 902,\n",
       " 'kinda': 903,\n",
       " 'citation': 904,\n",
       " 'supposed': 905,\n",
       " 'glass': 906,\n",
       " 'guys': 907,\n",
       " 'follow': 908,\n",
       " 'xkcd': 909,\n",
       " 'bet': 910,\n",
       " 'speak': 911,\n",
       " 'viewpoint': 912,\n",
       " 'agreed': 913,\n",
       " 'audio': 914,\n",
       " '72': 915,\n",
       " 'estimate': 916,\n",
       " 'original': 917,\n",
       " 'amount': 918,\n",
       " '01': 919,\n",
       " 'tom': 920,\n",
       " \"weren't\": 921,\n",
       " 'scotland': 922,\n",
       " 'fucked': 923,\n",
       " 'welcome': 924,\n",
       " 'beer': 925,\n",
       " 'drinking': 926,\n",
       " 'green': 927,\n",
       " 'selling': 928,\n",
       " 'uh': 929,\n",
       " 'payer': 930,\n",
       " \"state'\": 931,\n",
       " 'upvote': 932,\n",
       " 'voting': 933,\n",
       " 'view': 934,\n",
       " 'hate': 935,\n",
       " 'sacrifice': 936,\n",
       " 'sake': 937,\n",
       " 'games': 938,\n",
       " 'harass': 939,\n",
       " 'sucks': 940,\n",
       " 'changes': 941,\n",
       " 'evil': 942,\n",
       " 'diversion': 943,\n",
       " 'february': 944,\n",
       " 'san': 945,\n",
       " 'saddam': 946,\n",
       " 'found': 947,\n",
       " 'claims': 948,\n",
       " 'report': 949,\n",
       " 'hour': 950,\n",
       " 'mistakes': 951,\n",
       " 'came': 952,\n",
       " 'definition': 953,\n",
       " 'memory': 954,\n",
       " 'losers': 955,\n",
       " 'normal': 956,\n",
       " 'liberals': 957,\n",
       " 'hilarious': 958,\n",
       " 'model': 959,\n",
       " 'dvd': 960,\n",
       " 'fluid': 961,\n",
       " 'ounces': 962,\n",
       " 'index': 963,\n",
       " 'rich': 964,\n",
       " \"kucinich's\": 965,\n",
       " 'republicans': 966,\n",
       " 'raise': 967,\n",
       " 'pounds': 968,\n",
       " 'virtual': 969,\n",
       " 'shark': 970,\n",
       " 'boat': 971,\n",
       " 'salsa': 972,\n",
       " 'agreement': 973,\n",
       " 'ways': 974,\n",
       " 'elsewhere': 975,\n",
       " 'decision': 976,\n",
       " 'paying': 977,\n",
       " 'decisions': 978,\n",
       " 'design': 979,\n",
       " 'decide': 980,\n",
       " 'killer': 981,\n",
       " 'street': 982,\n",
       " 'name': 983,\n",
       " 'crazy': 984,\n",
       " 'bastard': 985,\n",
       " 'prize': 986,\n",
       " \"they'll\": 987,\n",
       " 'future': 988,\n",
       " 'dunno': 989,\n",
       " 'closer': 990,\n",
       " 'quarter': 991,\n",
       " 'scared': 992,\n",
       " 'break': 993,\n",
       " 'apply': 994,\n",
       " 'admire': 995,\n",
       " 'fan': 996,\n",
       " 'lives': 997,\n",
       " 'anybody': 998,\n",
       " 'zoom': 999,\n",
       " 'program': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(sequencesInput.shape)\n",
    "print(sequencesTarget.shape)\n",
    "Targetindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,  243,   28,    1,\n",
       "       1532,    8, 1533,  244,   14, 1534,    7, 1535,   89,  244,   41,\n",
       "         51,    3,   18,  922,  923, 1536,    8, 1537,   14,  198, 1538,\n",
       "         91])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequencesInput[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequencesInput[10, 90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4304"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Inputindex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-109-3b88e2a5979d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msequencesInput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mInputindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "indices = np.arange(sequencesInput)(Inputindex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4304\n",
      "4129\n",
      "373\n",
      "913\n"
     ]
    }
   ],
   "source": [
    "num_encoder_tokens = len(Inputindex)\n",
    "num_decoder_tokens = len(Targetindex)\n",
    "max_encoder_seq_length = max([len(txt) for txt in trainInputTokens])\n",
    "max_decoder_seq_length = max([len(txt) for txt in trainTargetTokens])\n",
    "\n",
    "print(num_encoder_tokens)\n",
    "print(num_decoder_tokens)\n",
    "print(max_encoder_seq_length)\n",
    "print(max_decoder_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "178"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Targetindex['also']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "glove_dir = r'C:\\Users\\mosta\\Desktop\\Deep Learning Projects\\Projects\\Lab Exercises\\Machine Learning Projects\\glove.6B\\glove.6B.300d.txt'\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(glove_dir, encoding=\"utf8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Embedding layer takes at least two arguments:\n",
    "# the number of possible tokens(the maximum word index)\n",
    "# and the dimensionality of the embeddings, here 300.\n",
    "embedding_dim = 300\n",
    "\n",
    "embedding_Inputmatrix = np.zeros((max_words, embedding_dim))\n",
    "for word, i in Inputindex.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if i < max_words:\n",
    "        if embedding_vector is not None:\n",
    "            # Words not found in embedding index will be all-zeros.\n",
    "            embedding_Inputmatrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_Targetmatrix = np.zeros((max_words, embedding_dim))\n",
    "for word, i in Targetindex.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if i < max_words:\n",
    "        if embedding_vector is not None:\n",
    "            # Words not found in embedding index will be all-zeros.\n",
    "            embedding_Targetmatrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder / Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 300)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_Inputmatrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_Inputmatrix[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Fit Data into Encoder / Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 100, 4304)\n",
      "(1000, 100, 4129)\n",
      "(1000, 100, 4129)\n"
     ]
    }
   ],
   "source": [
    "encoder_input_data = np.zeros((len(trainInputTokens), max_len, num_encoder_tokens), dtype='float32')\n",
    "decoder_input_data = np.zeros((len(trainTargetTokens), max_len, num_decoder_tokens), dtype='float32')\n",
    "decoder_target_data = np.zeros((len(trainTargetTokens), max_len, num_decoder_tokens), dtype='float32')\n",
    "\n",
    "print(encoder_input_data.shape)\n",
    "print(decoder_input_data.shape)\n",
    "print(decoder_target_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input_data[10, 10, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['does', 'this', 'also', 'apply', 'to']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainFromWordTokens[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-106-ef28ce0ef1e7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrainInputTokens\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrainFromSentenceTokens\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m             \u001b[0mencoder_input_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mInputindex\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "for i in trainInputTokens:\n",
    "    for t in range (0, max_len):\n",
    "        for word in trainFromSentenceTokens[i]:\n",
    "            encoder_input_data[i, t, Inputindex[word]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "newEncoder = np.zeros((len(trainInputTokens), max_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "newEncoder = sequencesInput[:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newEncoder[10, 90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "neEncoder1 = newEncoder[:, :, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 100, 1)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neEncoder1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot resize this array: it does not own its data",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-90-7a86b31e3ba6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mneEncoder1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: cannot resize this array: it does not own its data"
     ]
    }
   ],
   "source": [
    "neEncoder1.resize(1000, 100, 4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 100000 into shape (1000,100,4200)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-86-cf47d991cfd6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mneEncoder1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4200\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 100000 into shape (1000,100,4200)"
     ]
    }
   ],
   "source": [
    "neEncoder1.reshape(1000, 100, 4200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'numpy.ndarray' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-100-544d1d01ac08>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mneEncoder2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnewEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mInputindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'numpy.ndarray' object is not callable"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-598a1689ed39>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchar\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainInputTokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0mencoder_input_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTargetindex\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mchar\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mencoder_input_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTargetindex\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m: 2"
     ]
    }
   ],
   "source": [
    "for i, (trainInputTokens, trainTargetTokens) in enumerate(zip(trainInputTokens, trainTargetTokens)):\n",
    "    for t, char in enumerate(trainInputTokens):\n",
    "        encoder_input_data[i, t, Targetindex[char]] = 1.\n",
    "    encoder_input_data[i, t + 1:, Targetindex[]] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 300\n",
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=32,\n",
    "          epochs=2,\n",
    "          validation_split=0.2)\n",
    "# Save model\n",
    "model.save('s2s.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_token_index = dict([(char, i) for i, char in enumerate(trainFromWordTokens)])\n",
    "target_token_index = dict([(char, i) for i, char in enumerate(trainToWordTokens)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_token_index['hello']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (trainFromWordTokens, trainToWordTokens) in enumerate(zip(trainFromWordTokens, trainToWordTokens)):\n",
    "    for t, char in enumerate(trainFromWordTokens):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.\n",
    "    encoder_input_data[i, t + 1:, input_token_index[' ']] = 1.\n",
    "    for t, char in enumerate(trainToWordTokens):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.\n",
    "    decoder_input_data[i, t + 1:, target_token_index[' ']] = 1.\n",
    "    decoder_target_data[i, t:, target_token_index[' ']] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(input_characters)])\n",
    "target_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(target_characters)])"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
