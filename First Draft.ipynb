{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.4'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "import re\n",
    "import numpy as np\n",
    "from keras import preprocessing\n",
    "from keras import optimizers\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import SeparableConv1D, MaxPooling1D\n",
    "from keras.layers import LSTM, GRU\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Flatten\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 50294 words.\n",
      "Found 50284 sequences.\n"
     ]
    }
   ],
   "source": [
    "# load ascii text and covert to lowercase\n",
    "filename = \"GreatGatsby.txt\"\n",
    "text = open(filename, 'r', encoding='utf-8').read().lower()\n",
    "tokens = text.split(' ')\n",
    "tokens = re.split(' |\\n', text)\n",
    "\n",
    "\n",
    "train_len = 10\n",
    "text_sequences = []\n",
    "for i in range(train_len,len(tokens)):\n",
    "    seq = tokens[i-train_len:i]\n",
    "    text_sequences.append(seq)\n",
    "\n",
    "print('Found %s words.' %len(tokens))\n",
    "print('Found %s sequences.' %len(text_sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['younger',\n",
       " 'and',\n",
       " 'more',\n",
       " 'vulnerable',\n",
       " 'years',\n",
       " 'my',\n",
       " 'father',\n",
       " 'gave',\n",
       " 'me',\n",
       " 'some']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_sequences[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9049 unique tokens.\n",
      "Found 9049 unique words.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(text_sequences)\n",
    "sequences = tokenizer.texts_to_sequences(text_sequences)\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=train_len)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "word_count = tokenizer.word_counts\n",
    "nWords     = len(tokenizer.word_counts) + 1\n",
    "\n",
    "n_sequences = np.empty([len(sequences),train_len], dtype='int32')\n",
    "for i in range(len(sequences)):\n",
    "    n_sequences[i] = sequences[i]\n",
    "\n",
    "    \n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "print('Found %s unique words.' % len(word_count))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50284, 9)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "train_inputs = n_sequences[:,:-1]\n",
    "train_targets = n_sequences[:,-1]\n",
    "\n",
    "train_targets = to_categorical(train_targets, num_classes=len(word_count)+1)\n",
    "seq_len = train_inputs.shape[1]\n",
    "train_inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50284, 9050)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "glove_dir = r'C:\\Users\\mosta\\Desktop\\Deep Learning Projects\\Projects\\Lab Exercises\\Machine Learning Projects\\glove.6B\\glove.6B.300d.txt'\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(glove_dir, encoding=\"utf8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Embedding layer takes at least two arguments:\n",
    "# the number of possible tokens, the maximum word index,\n",
    "# and the dimensionality of the embeddings, here 300.\n",
    "embedding_dim = 300\n",
    "\n",
    "embedding_matrix = np.zeros((nWords, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if i < nWords:\n",
    "        if embedding_vector is not None:\n",
    "            # Words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(nWords, embedding_dim, input_length=seq_len))\n",
    "model.add(LSTM(50,return_sequences=True))\n",
    "model.add(LSTM(50))\n",
    "model.add(Dense(50,activation='relu'))\n",
    "model.add(Dense(nWords,activation='softmax'))\n",
    "opt_adam = optimizers.adam(lr=0.001)\n",
    "model.compile(loss='categorical_crossentropy',optimizer=opt_adam,metrics=['accuracy'])\n",
    "model.summary()\n",
    "model.layers[0].set_weights([embedding_matrix])\n",
    "model.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(nWords, embedding_dim, input_length=seq_len))\n",
    "model.add(SeparableConv1D(32, 9, activation='relu'))\n",
    "model.add(MaxPooling1D(1))\n",
    "model.add(SeparableConv1D(32, 1, activation='relu'))\n",
    "model.add(Bidirectional(GRU(32, dropout=0.1, recurrent_dropout=0.5, return_sequences=True)))\n",
    "model.add(GRU(32, dropout=0.1, recurrent_dropout=0.5,return_sequences=True))\n",
    "model.add(GRU(32, dropout=0.1, recurrent_dropout=0.5))\n",
    "model.add(Dense(nWords,activation='softmax'))\n",
    "opt_adam = optimizers.adam(lr=0.001)\n",
    "model.compile(loss='categorical_crossentropy',optimizer=opt_adam,metrics=['accuracy'])\n",
    "model.summary()\n",
    "model.layers[0].set_weights([embedding_matrix])\n",
    "model.layers[0].trainable = False\n",
    "\n",
    "#model.add(Dropout(0.2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\mosta\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mosta\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\mosta\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\mosta\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "Epoch 1/10\n",
      "50284/50284 [==============================] - 83s 2ms/step - loss: 7.2560 - acc: 0.0457\n",
      "\n",
      "Epoch 00001: loss improved from inf to 7.25602, saving model to C:\\Users\\mosta\\Desktop\\Deep Learning Projects\\Projects\\Lib\\word_pred_Model4.h5\n",
      "Epoch 2/10\n",
      "50284/50284 [==============================] - 78s 2ms/step - loss: 6.7834 - acc: 0.0468\n",
      "\n",
      "Epoch 00002: loss improved from 7.25602 to 6.78342, saving model to C:\\Users\\mosta\\Desktop\\Deep Learning Projects\\Projects\\Lib\\word_pred_Model4.h5\n",
      "Epoch 3/10\n",
      "50284/50284 [==============================] - 72s 1ms/step - loss: 6.5114 - acc: 0.0651\n",
      "\n",
      "Epoch 00003: loss improved from 6.78342 to 6.51140, saving model to C:\\Users\\mosta\\Desktop\\Deep Learning Projects\\Projects\\Lib\\word_pred_Model4.h5\n",
      "Epoch 4/10\n",
      "50284/50284 [==============================] - 71s 1ms/step - loss: 6.2516 - acc: 0.0873\n",
      "\n",
      "Epoch 00004: loss improved from 6.51140 to 6.25164, saving model to C:\\Users\\mosta\\Desktop\\Deep Learning Projects\\Projects\\Lib\\word_pred_Model4.h5\n",
      "Epoch 5/10\n",
      "50284/50284 [==============================] - 75s 1ms/step - loss: 6.0157 - acc: 0.0987\n",
      "\n",
      "Epoch 00005: loss improved from 6.25164 to 6.01565, saving model to C:\\Users\\mosta\\Desktop\\Deep Learning Projects\\Projects\\Lib\\word_pred_Model4.h5\n",
      "Epoch 6/10\n",
      "50284/50284 [==============================] - 74s 1ms/step - loss: 5.7967 - acc: 0.1103\n",
      "\n",
      "Epoch 00006: loss improved from 6.01565 to 5.79666, saving model to C:\\Users\\mosta\\Desktop\\Deep Learning Projects\\Projects\\Lib\\word_pred_Model4.h5\n",
      "Epoch 7/10\n",
      "50284/50284 [==============================] - 74s 1ms/step - loss: 5.5892 - acc: 0.1225\n",
      "\n",
      "Epoch 00007: loss improved from 5.79666 to 5.58919, saving model to C:\\Users\\mosta\\Desktop\\Deep Learning Projects\\Projects\\Lib\\word_pred_Model4.h5\n",
      "Epoch 8/10\n",
      "50284/50284 [==============================] - 76s 2ms/step - loss: 5.3848 - acc: 0.1400\n",
      "\n",
      "Epoch 00008: loss improved from 5.58919 to 5.38477, saving model to C:\\Users\\mosta\\Desktop\\Deep Learning Projects\\Projects\\Lib\\word_pred_Model4.h5\n",
      "Epoch 9/10\n",
      "50284/50284 [==============================] - 79s 2ms/step - loss: 5.1895 - acc: 0.1601\n",
      "\n",
      "Epoch 00009: loss improved from 5.38477 to 5.18955, saving model to C:\\Users\\mosta\\Desktop\\Deep Learning Projects\\Projects\\Lib\\word_pred_Model4.h5\n",
      "Epoch 10/10\n",
      "50284/50284 [==============================] - 75s 1ms/step - loss: 5.0210 - acc: 0.1757\n",
      "\n",
      "Epoch 00010: loss improved from 5.18955 to 5.02100, saving model to C:\\Users\\mosta\\Desktop\\Deep Learning Projects\\Projects\\Lib\\word_pred_Model4.h5\n"
     ]
    }
   ],
   "source": [
    "from pickle import dump\n",
    "from pickle import load\n",
    "from keras.models import load_model\n",
    "path = r'C:\\Users\\mosta\\Desktop\\Deep Learning Projects\\Projects\\Lib\\word_pred_Model4.h5'\n",
    "checkpoint = ModelCheckpoint(path, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "model.fit(train_inputs,train_targets,batch_size=32,epochs=10,verbose=1,callbacks=[checkpoint])\n",
    "model.save('word_pred_Model4.h5')\n",
    "dump(tokenizer,open('tokenizer_Model4','wb')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "    \n",
    "sampled = encoded_text?\n",
    "preds = model.predict(sampled, verbose=0)[0]\n",
    "next_index = sample(preds, temperature)\n",
    "next_char = chars[next_index]\n",
    "\n",
    "generated_text += next_char\n",
    "generated_text = generated_text[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_sequence(model, tokenizer, seq_len, seed_text, n_words):\n",
    "    output_text = []\n",
    "    input_text = seed_text\n",
    "    \n",
    "    for i in range(n_words):\n",
    "        encoded_text = tokenizer.texts_to_sequences([input_text])[0]\n",
    "        encoded_text = pad_sequences([encoded_text], maxlen=seq_len, truncating='pre')\n",
    "        prediction   = model.predict_classes(encoded_text, verbose=0)[0]\n",
    "        \n",
    "        pred_word    = tokenizer.index_word[prediction]\n",
    "        input_text   += ' '+ pred_word\n",
    "        output_text.append(pred_word)\n",
    "        \n",
    "    return ' '.join(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-e72cf8f7a229>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'word_pred_Model4.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'tokenizer_Model4'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n\\n===>Enter --exit to exit from the program'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mseed_text\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Enter string: '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'load_model' is not defined"
     ]
    }
   ],
   "source": [
    "model = load_model('word_pred_Model4.h5')\n",
    "tokenizer = load(open('tokenizer_Model4','rb'))\n",
    "print('\\n\\n===>Enter --exit to exit from the program')\n",
    "while True:\n",
    "    seed_text  = input('Enter string: ')\n",
    "    if seed_text.lower() == '--exit':\n",
    "        break\n",
    "    else:\n",
    "        out = gen_sequence(model, tokenizer, seq_len=seq_len, seed_text=seed_text, n_words=3)\n",
    "        print('Output: '+' '+out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
